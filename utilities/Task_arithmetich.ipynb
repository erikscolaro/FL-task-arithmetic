{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TjdKXd_Nwfxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "What\\'' s task arithmetic?"
      ],
      "metadata": {
        "id": "RWFBr2cz441m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This first snippet comes from the TaLoS pruner.\n",
        "\n",
        "A Pruner is a class that takes in a model a testing dataset and finds the parameters that change the less (so are less relevant for the task we are doing).\n",
        "\n",
        "This knowledge usually is used to cut the dimension of the network.\n",
        "\n",
        "TaLoS on the other hand wants to find the parameters responsible for identifiying a particular class.\n",
        "\n",
        "This is fondamental to do task arithmetic. For example, if we fine-tune a model to recognize dogs, with TaLoS we can find the parameters that are important to do this task. We can do the same with cats.\n",
        "\n",
        "In the end we can get a model that recognizes cats and dogs by simply injecting in a vanilla model, the parameters we gathered with TaLoS for dogs and cats."
      ],
      "metadata": {
        "id": "w8u-uZadzYA2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the first code snippet the TA gave us. It's inside the score funtion of the TaLoS pruner. Its job is to compute the diagonal elements of the Fisher Information matrix.\n",
        "\n",
        "What does it mean? Every time the model is given a picture the backpropagation changes our parameters. TaLoS calculates the gradient of this change, squares it(to have a positive modulus) and adds it to previous square gradients.\n",
        "\n",
        "So in the end for every parameter we will have the squared sum of his gradients. The ones with smaller modulus will be the less significat parameters for our tasks, while the ones with largest modules are the parameters that tuned and \"learned\" to do the task we requested."
      ],
      "metadata": {
        "id": "Qx2cV57E7HBY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7imhW1zwdfO"
      },
      "outputs": [],
      "source": [
        "for _ in range(self.R):\n",
        "                #Here we get a tensor with values of the last layer of the model after it was given a bunch of picures(our input to learn a task)\n",
        "                logits = model(input)\n",
        "                #Here We do a softmax of this tensor, so that every value becomes a probability and their sum adds to 1 (e.g {[90, 10]} becomes [0.9, 0.1])\n",
        "                outdx = torch.distributions.Categorical(logits=logits)\n",
        "                #.sample() will spin a roulette with every class having his softmax probability to come out (for [0.9, 0.1], if we run on it .sample() 10 times, we will get\n",
        "                # [0] 9 times and [1] 1 time. Unsqueeze just changes the shape of the tensor and detach tells Pytorch to stop traking the gradients\n",
        "                .sample().unsqueeze(1).detach()\n",
        "                #Now, outdx is an array of indexes. For every picture we know what class .sample() chose for us.\n",
        "                # .gather(dim,idx) for the chosen dimension (0 = columns, 1 rows) will take the corrispective index. idx is a vector matching the dim we gave.\n",
        "                # In our example, logits = [90,10], outx = [1], samples = logits.gather(0,[1]) ===> sample = [90], if outx = [0] ====> sample = [10]\n",
        "                samples = logits.gather(1, outdx)\n",
        "\n",
        "                for idx in range(data.size(0)):\n",
        "                    #We don't track the model gradient (we don't need to compute the backpropagation, we already did model(input))\n",
        "                    model.zero_grad()\n",
        "                    #This computes the derivative of that specific output with respect to every weight in the network.\n",
        "                    # sample[idx] ,as we explained before, is the the output value of the network for a specific picture\n",
        "                    torch.autograd.backward(samples[idx], retain_graph=True)\n",
        "                    # masked_parameters(model) returns all the parameters responsible for defining Linear connections or Convolutional connections, ignoring the others\n",
        "                    for m, p in masked_parameters(model):\n",
        "                        if p.requires_grad and hasattr(p, 'grad') and p.grad is not None:\n",
        "                            # Here we compute, step by step, the diagonal elements of the Fisher Information matrix by summing the squares of the gradients\n",
        "                            self.scores[id(p)] += torch.clone(p.grad.data.pow(2)).detach().cpu()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rememeber the previous snippet was inside the TaLoS score() function in which we compute Fisher's diagonal Matrix.\n",
        "\n",
        "We can now understand the second snippet the TA gave to us."
      ],
      "metadata": {
        "id": "lyY7TR5LEPQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Sparsity is just the fraction of the network we want to retain. Sparsity 0.5, we retain half of the network and prune the other, less significant, half\n",
        "if sparsity < 1.0:\n",
        "        #In the complete code of TaLoS they try to prune for multiple rounds I think for research porupese\n",
        "        for round in range(ROUNDS):\n",
        "            # Only in the finale round sparse = sparsity\n",
        "            sparse = sparsity**((round + 1) / ROUNDS)\n",
        "            print('[+] Target sparsity:', sparse)\n",
        "            #We compute the Fisher diagonal Matrix as shown before\n",
        "            pruner.score(model, None, data_loader, args.device, N_PRUNING_BATCHES)\n",
        "            #Inside the mask() function there is a Switch to choose differents ways to compute the threshold for pruning/masking or not a parameter\n",
        "            mode = 'global_copy'\n",
        "            #pruner.mask() takes the score of our model and based on them and the mode chooses which parameters to mask.\n",
        "            # From a practical point of view, mask() changes the self.masked_parameters which is an iterator that returns (mask, param) where mask equals 1 or 0, while param is the parameter\n",
        "            #Where does the pruner gets the model scores? Remember the pruner is wrapped around the model itself(the model is an __init__ parameter)\n",
        "\n",
        "            pruner.mask(sparse, mode)"
      ],
      "metadata": {
        "id": "iLrwWoZzEOuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After seeing how to find the most significant parameters for a task, we can implement the SparseSGDM that we were asked.\n",
        "\n",
        "The optimizer takes already normally the parameters as input. Every parameter has his mask saved in  self.state[parameter]['mask']\n",
        "\n",
        "The only addition we make to a normal SGDM is in point 2. MASKING GRADIENT.\n",
        "\n",
        "With this Optimizer we will be able to train/fine tune only some parts of our model. This parts can be identified with TaLoS or other pruning tools."
      ],
      "metadata": {
        "id": "_hO7CFDMHpe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.optim import Optimizer\n",
        "\n",
        "class SparseSGDM(Optimizer):\n",
        "    r\"\"\"\n",
        "    Implements Stochastic Gradient Descent with Momentum (SGDM)\n",
        "    where the gradient mask is passed directly into the constructor.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, masks=None, lr=1e-3, momentum=0, dampening=0,\n",
        "                 weight_decay=0, nesterov=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            params (iterable): iterable of parameters to optimize (usually model.parameters())\n",
        "            masks (iterable, optional): iterable of (mask, param) tuples.\n",
        "                                        (in case we use the TaLoS masked_parameters(model))\n",
        "            lr (float): learning rate\n",
        "            momentum (float, optional): momentum factor (default: 0)\n",
        "            weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
        "            dampening (float, optional): dampening for momentum (default: 0)\n",
        "            nesterov (bool, optional): enables Nesterov momentum (default: False)\n",
        "        \"\"\"\n",
        "        if lr < 0.0:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if momentum < 0.0:\n",
        "            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n",
        "        if weight_decay < 0.0:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "\n",
        "        defaults = dict(lr=lr, momentum=momentum, dampening=dampening,\n",
        "                        weight_decay=weight_decay, nesterov=nesterov)\n",
        "\n",
        "        # 1. Initialize the Base Optimizer with the parameters\n",
        "        super(SparseSGDM, self).__init__(params, defaults)\n",
        "\n",
        "        # 2. Automatically register masks if provided in constructor\n",
        "        if masks is not None:\n",
        "            self._register_masks(masks)\n",
        "\n",
        "    def _register_masks(self, masked_parameters):\n",
        "        \"\"\"\n",
        "        Internal helper to populate self.state with masks.\n",
        "        \"\"\"\n",
        "        for mask, param in masked_parameters:\n",
        "            # Initialize state if it implies lazy initialization\n",
        "            if param not in self.state:\n",
        "                self.state[param] = {}\n",
        "\n",
        "            # Store mask in the specific parameter's state dictionary\n",
        "            # We ensure device consistency (move mask to CPU/GPU of the param)\n",
        "            self.state[param]['mask'] = mask.to(param.device)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step with Masking.\"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            weight_decay = group['weight_decay']\n",
        "            momentum = group['momentum']\n",
        "            dampening = group['dampening']\n",
        "            nesterov = group['nesterov']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                # the name d_p resembles \"delta p\" of the gradient\n",
        "                d_p = p.grad\n",
        "\n",
        "                # Retrieve mask from state\n",
        "                state = self.state[p]\n",
        "                mask = state.get('mask')\n",
        "\n",
        "                # 1. Apply Weight Decay (Before masking)\n",
        "                if weight_decay != 0:\n",
        "                    d_p = d_p.add(p, alpha=weight_decay)\n",
        "\n",
        "                # 2. MASKING GRADIENT: Prevent update from current batch\n",
        "                if mask is not None:\n",
        "                    d_p.mul_(mask)\n",
        "\n",
        "                # 3. Momentum Logic\n",
        "                if momentum != 0:\n",
        "                    if 'momentum_buffer' not in state:\n",
        "                        buf = state['momentum_buffer'] = torch.clone(d_p).detach()\n",
        "                    else:\n",
        "                        buf = state['momentum_buffer']\n",
        "                        buf.mul_(momentum).add_(d_p, alpha=1 - dampening)\n",
        "\n",
        "                    # 4. MASKING MOMENTUM: Prevent drift from history\n",
        "                    if mask is not None:\n",
        "                        buf.mul_(mask)\n",
        "\n",
        "                    if nesterov:\n",
        "                        d_p = d_p.add(buf, alpha=momentum)\n",
        "                    else:\n",
        "                        d_p = buf\n",
        "\n",
        "                # 5. Update Weight\n",
        "                p.add_(d_p, alpha=-group['lr'])\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "yRtR8NeCH3_e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}