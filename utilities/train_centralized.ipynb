{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f7d5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor, Resize, CenterCrop\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from utilities.wandb_utils import load_checkpoint_from_wandb, save_checkpoint_to_wandb\n",
    "from fl_task_arithmetic.model import CustomDino\n",
    "\n",
    "\n",
    "ENTITY = \"aml-fl-project\"\n",
    "PROJECT = \"fl-task-arithmetic\"\n",
    "GROUP = \"centralized-dino-cifar100\"\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "LR  = 0.01           # Learning Rate (Tune this: 0.1, 0.01, 0.001)\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 5e-4\n",
    "EPOCHS = 100         # Increase to 100+ for final results\n",
    "DEVICE = torch.device(\"mps\") # torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "PATIENCE = 3\n",
    "\n",
    "# Standard CIFAR-100 Normalization\n",
    "stats = ((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
    "\n",
    "# Transforms\n",
    "transform_train = Compose([\n",
    "    Resize(256), CenterCrop(224), # Required for DINO\n",
    "    # transforms.RandomHorizontalFlip(), # Optional augmentation\n",
    "    ToTensor(),\n",
    "    Normalize(*stats),\n",
    "])\n",
    "\n",
    "transform_test = Compose([\n",
    "    Resize(256), CenterCrop(224),\n",
    "    ToTensor(),\n",
    "    Normalize(*stats),\n",
    "])\n",
    "\n",
    "trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
    "testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "testloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "def train(lr, momentum, weight_decay, epochs, scheduler_name, scheduler_fn):\n",
    "    best_accuracy = 0.0\n",
    "    patience_counter = 0\n",
    "    run_id = f\"run-1-centralized-dino-icarl-cifar100-lr{lr}-mom{momentum}-wd{weight_decay}-sched-{scheduler_name}\"\n",
    "    run = wandb.init(\n",
    "        entity=ENTITY,\n",
    "        project=PROJECT,\n",
    "        group=GROUP,\n",
    "        name=f\"centralized-dino-icarl-cifar100-lr{lr}-mom{momentum}-wd{weight_decay}-sched-{scheduler_name}\",\n",
    "        id=run_id,\n",
    "        resume=\"allow\",\n",
    "        mode=\"online\",\n",
    "    )\n",
    "\n",
    "    model = CustomDino().to(DEVICE)\n",
    "\n",
    "    checkpoint = load_checkpoint_from_wandb(\n",
    "        run,\n",
    "        model,\n",
    "        \"model.pth\"\n",
    ")\n",
    "    start_epoch = 0\n",
    "    if checkpoint is not None:\n",
    "        checkpoint_dict, artifact = checkpoint\n",
    "        model.load_state_dict(checkpoint_dict['model'])\n",
    "        start_epoch = artifact.metadata[\"epoch\"] + 1\n",
    "        print(f\"Resuming from epoch {start_epoch}\")\n",
    "    else:\n",
    "        print(\"Starting from scratch\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.backbone.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    scheduler = scheduler_fn(optimizer)\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    test_accs = []\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Progress bar for training\n",
    "        pbar = tqdm(trainloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_train_loss = running_loss / len(trainloader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # 5. Evaluation\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in testloader:\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        avg_test_loss = test_loss / len(testloader)\n",
    "        acc = 100. * correct / total\n",
    "\n",
    "        test_losses.append(avg_test_loss)\n",
    "        test_accs.append(acc)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} Results: Train Loss: {avg_train_loss:.4f} | Test Loss: {avg_test_loss:.4f} | Test Acc: {acc:.2f}%\")\n",
    "\n",
    "        if (acc > best_accuracy):\n",
    "            best_accuracy = acc\n",
    "            save_checkpoint_to_wandb(run, {\n",
    "                'model': model.state_dict(),\n",
    "            }, f\"model.pth\", {\n",
    "                \"task\": model,\n",
    "                \"accuracy\": acc,\n",
    "                \"epoch\": epoch\n",
    "            })\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter > PATIENCE:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "        print(epoch, \"Saved checkpoint model to WandB.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf3afac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madrientrahan\u001b[0m (\u001b[33maml-fl-project\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/adrientrahan/Documents/ecole/AML/project/FL-task-arithmetic/notebooks/wandb/run-20251213_173554-run-1-centralized-dino-icarl-cifar100-lr0.01-mom0.9-wd0.0005-sched-CosineAnnealingLR</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/aml-fl-project/fl-task-arithmetic/runs/run-1-centralized-dino-icarl-cifar100-lr0.01-mom0.9-wd0.0005-sched-CosineAnnealingLR' target=\"_blank\">centralized-dino-icarl-cifar100-lr0.01-mom0.9-wd0.0005-sched-CosineAnnealingLR</a></strong> to <a href='https://wandb.ai/aml-fl-project/fl-task-arithmetic' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aml-fl-project/fl-task-arithmetic' target=\"_blank\">https://wandb.ai/aml-fl-project/fl-task-arithmetic</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aml-fl-project/fl-task-arithmetic/runs/run-1-centralized-dino-icarl-cifar100-lr0.01-mom0.9-wd0.0005-sched-CosineAnnealingLR' target=\"_blank\">https://wandb.ai/aml-fl-project/fl-task-arithmetic/runs/run-1-centralized-dino-icarl-cifar100-lr0.01-mom0.9-wd0.0005-sched-CosineAnnealingLR</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/adrientrahan/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artifact membership 'centralized-dino-cifar100-checkpoints:latest' not found in 'aml-fl-project/fl-task-arithmetic'\n",
      "Model checkpoint not found on WandB. artifact membership 'centralized-dino-cifar100-checkpoints:latest' not found in 'aml-fl-project/fl-task-arithmetic'\n",
      "Starting from scratch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   1%|          | 7/782 [00:10<12:50,  1.01it/s, loss=7.56]  "
     ]
    }
   ],
   "source": [
    "\n",
    "SCHEDULERS = [\n",
    "    (\"CosineAnnealingLR\", lambda opt: torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS)),\n",
    "    (\"StepLR\", lambda opt: torch.optim.lr_scheduler.StepLR(opt, step_size=30, gamma=0.1)),\n",
    "    (\"NoScheduler\", lambda opt: torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda=lambda epoch: 1.0)) # No scheduling\n",
    "]\n",
    "    \n",
    "\n",
    "for scheduler_name, scheduler_fn in SCHEDULERS:\n",
    "    train(\n",
    "        lr=LR,\n",
    "        momentum=MOMENTUM,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        epochs=EPOCHS, \n",
    "        scheduler_name=scheduler_name, \n",
    "        scheduler_fn=scheduler_fn\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f026b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATES = [0.1, 0.01, 0.001]\n",
    "\n",
    "for lr in LEARNING_RATES:\n",
    "    train(\n",
    "        lr=lr,\n",
    "        momentum=MOMENTUM,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        epochs=EPOCHS, \n",
    "        scheduler_name=SCHEDULERS[0][0], \n",
    "        scheduler_fn=SCHEDULERS[0][1]\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl-task-arithmetic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
