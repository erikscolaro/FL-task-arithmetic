{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456
    },
    "id": "1S98QELEEJeU",
    "outputId": "b6a26106-dc39-45e4-a837-5e114aca9cdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33merikscolaro31\u001b[0m (\u001b[33maml-fl-project\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/einrich99/Progetti/FL-task-arithmetic/notebooks/wandb/run-20251211_193415-2i4ap60b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aml-fl-project/fl-task-arithmetic/runs/2i4ap60b' target=\"_blank\">icarl-centroids</a></strong> to <a href='https://wandb.ai/aml-fl-project/fl-task-arithmetic' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aml-fl-project/fl-task-arithmetic' target=\"_blank\">https://wandb.ai/aml-fl-project/fl-task-arithmetic</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aml-fl-project/fl-task-arithmetic/runs/2i4ap60b' target=\"_blank\">https://wandb.ai/aml-fl-project/fl-task-arithmetic/runs/2i4ap60b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/einrich99/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 10 images per class for centroid computation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing centroids per class: 100%|██████████| 100/100 [00:28<00:00,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved centroids as linear weights to centroids_linear.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to WandB as artifact 'centroids-checkpoints'.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">icarl-centroids</strong> at: <a href='https://wandb.ai/aml-fl-project/fl-task-arithmetic/runs/2i4ap60b' target=\"_blank\">https://wandb.ai/aml-fl-project/fl-task-arithmetic/runs/2i4ap60b</a><br> View project at: <a href='https://wandb.ai/aml-fl-project/fl-task-arithmetic' target=\"_blank\">https://wandb.ai/aml-fl-project/fl-task-arithmetic</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251211_193415-2i4ap60b/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor, Resize, CenterCrop\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from typing import Optional, cast\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "from utilities.wandb_utils import save_checkpoint_to_wandb\n",
    "\n",
    "# ==========================================\n",
    "# 0. Configuration & Paths\n",
    "# ==========================================\n",
    "BATCH_SIZE = 64\n",
    "LR = 0.01\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 5e-4\n",
    "EPOCHS = 50\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "TOTAL_EXEMPLARS_VECTORS = 1000  # Total exemplar vectors to store across all classes\n",
    "FILENAME = \"centroids_linear.pth\"\n",
    "\n",
    "WANDB_ENTITY = \"aml-fl-project\"\n",
    "WANDB_PROJECT = \"fl-task-arithmetic\"\n",
    "WANDB_GROUP = \"centroids\"\n",
    "WANDB_RUN_NAME = \"icarl-centroids\"\n",
    "WANDB_NOTES = \"Centroids computed over 100 CIFAR100 classes\"\n",
    "WANDB_MODE = \"online\"\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 1. Model Definition (CustomDino)\n",
    "# ==========================================\n",
    "class CustomDino(nn.Module):\n",
    "    def __init__(self, num_classes: int = 100, backbone: Optional[nn.Module] = None):\n",
    "        super().__init__()\n",
    "        if backbone is None:\n",
    "            backbone = cast(\n",
    "                nn.Module,\n",
    "                torch.hub.load(\n",
    "                    \"facebookresearch/dino:main\", \"dino_vits16\", pretrained=True\n",
    "                ),\n",
    "            )\n",
    "        self.backbone: nn.Module = backbone\n",
    "        self.num_classes = num_classes\n",
    "        self.classifier = nn.Linear(384, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        features = self.backbone(x)  # [batch, 384]\n",
    "        logits = self.classifier(features)\n",
    "        return logits, features\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 2. iCaRL Logic Class\n",
    "# ==========================================\n",
    "class iCaRL:\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes=100,\n",
    "        memory_size=TOTAL_EXEMPLARS_VECTORS,\n",
    "        feature_dim=384,\n",
    "        device=\"cuda\",\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.num_classes = num_classes\n",
    "        self.memory_size = memory_size\n",
    "        self.feature_dim = feature_dim\n",
    "\n",
    "        # Initialize Model\n",
    "        self.model = CustomDino(num_classes=num_classes).to(self.device)\n",
    "        self.old_model = None  # Snapshot of model before current task\n",
    "\n",
    "        # Memory (Exemplars)\n",
    "        self.exemplar_sets = []  # List of lists (images per class)\n",
    "        self.exemplar_means = []  # Class prototypes for NME\n",
    "\n",
    "        # Training Parameters\n",
    "        self.lr = 0.01\n",
    "        self.weight_decay = 1e-5\n",
    "        self.momentum = 0.9\n",
    "        self.epochs = 20  # Reduced for demo speed (standard is often higher)\n",
    "\n",
    "    def update_representation(self, train_loader, new_classes):\n",
    "        \"\"\"\n",
    "        Step 1: Train the model using Classification + Distillation Loss\n",
    "        \"\"\"\n",
    "        print(f\"--- Updating Representation for classes {new_classes} ---\")\n",
    "\n",
    "        # 1. Combine new data with exemplars\n",
    "        # (In this simplified script, we assume train_loader already mixes them if available\n",
    "        # or we just iterate. For strict iCaRL, we augment the batch with exemplars).\n",
    "        # To keep it simple for Colab, we will rely on the DataLoader having the mix.\n",
    "\n",
    "        optimizer = optim.SGD(\n",
    "            self.model.parameters(),\n",
    "            lr=self.lr,\n",
    "            momentum=self.momentum,\n",
    "            weight_decay=self.weight_decay,\n",
    "        )\n",
    "        # Scheduler helps convergence\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=self.epochs)\n",
    "\n",
    "        self.model.train()\n",
    "        if self.old_model:\n",
    "            self.old_model.eval()\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            total_loss = 0\n",
    "            for images, labels in tqdm(\n",
    "                train_loader, desc=f\"Epoch {epoch+1}/{self.epochs}\", leave=False\n",
    "            ):\n",
    "                images = images.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward Pass\n",
    "                logits, _ = self.model(images)\n",
    "\n",
    "                # --- Loss Calculation ---\n",
    "                # A. Classification Loss (Cross Entropy on all visible classes)\n",
    "                loss_cls = F.cross_entropy(logits, labels)\n",
    "\n",
    "                # B. Distillation Loss (on OLD classes only)\n",
    "                loss_dist = torch.tensor(0.0).to(self.device)\n",
    "                if self.old_model is not None:\n",
    "                    # Get old logits\n",
    "                    with torch.no_grad():\n",
    "                        old_logits, _ = self.old_model(images)\n",
    "\n",
    "                    # Sigmoid Distillation (Rebuffi et al. 2017)\n",
    "                    # We compute BCE between the sigmoid outputs of the new model and the old model\n",
    "                    # solely for the classes the old model knew.\n",
    "                    known_classes = self.old_model.classifier.out_features\n",
    "                    # Usually iCaRL assumes specific output nodes. Here we map indices.\n",
    "                    # We assume indices 0 to (start of new task) are old classes.\n",
    "\n",
    "                    # Create a mask for old classes (e.g., 0 to 10, then 0 to 20...)\n",
    "                    # The 'old_logits' typically has size [B, num_classes] same as current if architecture is fixed\n",
    "                    # Or [B, old_num_classes] if it grew. DINO linear layer is usually fixed size or grows.\n",
    "                    # Here we assume fixed size 100 for simplicity.\n",
    "\n",
    "                    # Calculate Distillation:\n",
    "                    # T=1 is standard for iCaRL's sigmoid distillation\n",
    "                    # [:, :start_new_task] Are all the old classes the new model should not forget\n",
    "                    start_new_task = new_classes[0]\n",
    "                    if start_new_task > 0:\n",
    "                        dist_target = torch.sigmoid(old_logits[:, :start_new_task])\n",
    "                        dist_pred = torch.sigmoid(logits[:, :start_new_task])\n",
    "                        loss_dist = F.binary_cross_entropy(dist_pred, dist_target)\n",
    "\n",
    "                loss = loss_cls + loss_dist\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            scheduler.step()\n",
    "            # print(f\"Epoch {epoch}: Loss {total_loss:.4f}\")\n",
    "\n",
    "        # Update the frozen old model\n",
    "        self.old_model = deepcopy(self.model)\n",
    "        for p in self.old_model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def reduce_exemplar_sets(self, m):\n",
    "        \"\"\"\n",
    "        Step 2: Shrink stored exemplars to fit memory budget.\n",
    "        m = memory_size / num_classes_seen_so_far\n",
    "        \"\"\"\n",
    "        print(f\"Reducing exemplars to {m} per class...\")\n",
    "        for y in range(len(self.exemplar_sets)):\n",
    "            self.exemplar_sets[y] = self.exemplar_sets[y][:m]\n",
    "\n",
    "    def construct_exemplar_sets(self, images, m, transform, class_id):\n",
    "        \"\"\"\n",
    "        Step 3: Select new exemplars using Herding (nearest to mean).\n",
    "        \"\"\"\n",
    "        print(f\"Constructing {m} exemplars vectors per class number {class_id}\")\n",
    "        self.model.eval()\n",
    "\n",
    "        # Compute mean of the class\n",
    "        with torch.no_grad():\n",
    "            # Extract features\n",
    "            # Note: We need a loader to process 'images' (which is a list/tensor of raw images)\n",
    "            # For efficiency in this script, we assume 'images' fits in VRAM or we batch it.\n",
    "            # Simplified:\n",
    "            img_tensor = torch.stack(images).to(self.device)\n",
    "            _, features = self.model(img_tensor)\n",
    "            features = F.normalize(features, p=2, dim=1)\n",
    "            class_mean = torch.mean(features, dim=0)\n",
    "\n",
    "            # Herding Selection\n",
    "            exemplar_set = []\n",
    "            exemplar_features = []\n",
    "\n",
    "            # We assume features are [N, D]\n",
    "            # We iterate m times to pick m samples\n",
    "            for k in range(m):\n",
    "                S = (\n",
    "                    torch.sum(torch.stack(exemplar_features), dim=0)\n",
    "                    if len(exemplar_features) > 0\n",
    "                    else torch.zeros(self.feature_dim).to(self.device)\n",
    "                )\n",
    "\n",
    "                # Objective: minimize || class_mean - (S + phi(x)) / k   ||\n",
    "                phi = features  # [N, D]\n",
    "                mu = class_mean  # [D]\n",
    "\n",
    "                # Distance for all candidates\n",
    "                dists = torch.norm(mu - ((S + phi) / k), dim=1)\n",
    "\n",
    "                # Pick best that isn't already chosen (simple way: set dist to inf)\n",
    "                # In strict implementation, we remove the index.\n",
    "                best_idx = torch.argmin(dists).item()\n",
    "\n",
    "                exemplar_set.append(images[best_idx])\n",
    "                exemplar_features.append(features[best_idx])\n",
    "\n",
    "                # Mask this index so it's not picked again\n",
    "                features[best_idx] = features[best_idx] + 10000  # Hacky mask\n",
    "\n",
    "            self.exemplar_sets.append(exemplar_set)\n",
    "\n",
    "    def classify_nme(self, x):\n",
    "        \"\"\"\n",
    "        Step 4: Classification using Nearest Mean of Exemplars.\n",
    "        Strict Implementation of Algorithm 1 & Eq. 2\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            # 1. Get features of the image to classify\n",
    "            _, query_features = self.model(x.to(self.device))\n",
    "            # Normalize query features (Section 2.1)\n",
    "            query_features = F.normalize(query_features, p=2, dim=1)\n",
    "\n",
    "            # 2. Compute Prototypes (Means of Exemplars)\n",
    "            means = []\n",
    "            for y in range(len(self.exemplar_sets)):\n",
    "                # Get all exemplars for class y\n",
    "                ex_imgs = torch.stack(self.exemplar_sets[y]).to(self.device)\n",
    "\n",
    "                # Extract features for exemplars\n",
    "                _, ex_feats = self.model(ex_imgs)\n",
    "\n",
    "                # Normalize exemplar features BEFORE averaging (Section 2.1)\n",
    "                ex_feats = F.normalize(ex_feats, p=2, dim=1)\n",
    "\n",
    "                # Compute the mean\n",
    "                class_mean = torch.mean(ex_feats, dim=0)\n",
    "\n",
    "                # Re-normalize the mean vector itself (Section 2.1: \"averages are also re-normalized\")\n",
    "                class_mean = F.normalize(class_mean.unsqueeze(0), p=2, dim=1).squeeze(0)\n",
    "\n",
    "                means.append(class_mean)\n",
    "\n",
    "            if len(means) == 0:\n",
    "                return torch.zeros(x.size(0))\n",
    "\n",
    "            means = torch.stack(means)  # [Num_Classes_Seen, Feature_Dim]\n",
    "\n",
    "            # 3. Find Nearest Prototype (Algorithm 1)\n",
    "            # \"y* = argmin || phi(x) - mu_y ||\"\n",
    "            dists = torch.cdist(query_features, means)  # [Batch, Num_Classes]\n",
    "            preds = torch.argmin(dists, dim=1)\n",
    "\n",
    "        return preds\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 3. Checkpointing Functions\n",
    "# ==========================================\n",
    "def save_icarl_checkpoint(icarl_instance, task_id, acc, checkpoint_dir):\n",
    "    \"\"\"Saves the entire state required for iCaRL checkpointing.\"\"\"\n",
    "\n",
    "    # Construct the path\n",
    "    filename = f\"icarl_task_{task_id:02d}_acc_{acc:.2f}.pth\"\n",
    "    save_path = os.path.join(checkpoint_dir, filename)\n",
    "    # Data to save: model states, memory, and task metadata\n",
    "    checkpoint_data = {\n",
    "        \"task_id\": task_id,\n",
    "        \"accuracy\": acc,\n",
    "        # 'model_state_dict': icarl_instance.model.state_dict(),\n",
    "        # 'old_model_state_dict': icarl_instance.old_model.state_dict() if icarl_instance.old_model else None,\n",
    "        # Save the exemplar sets. We move tensors to CPU for better portability/storage.\n",
    "        \"exemplar_sets\": [\n",
    "            [img.cpu() for img in class_set]\n",
    "            for class_set in icarl_instance.exemplar_sets\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    print(f\"\\n Saving checkpoint for Task {task_id} (Acc: {acc:.2f}%) to: {save_path}\")\n",
    "    torch.save(checkpoint_data, save_path)\n",
    "\n",
    "    # Optional: Keep only the latest checkpoint to save space\n",
    "    # You might comment this out if you want to keep all task checkpoints\n",
    "    for file in os.listdir(checkpoint_dir):\n",
    "        if file.startswith(\"icarl_task\") and file != filename:\n",
    "            os.remove(os.path.join(checkpoint_dir, file))\n",
    "            # print(f\"Cleaned up old checkpoint: {file}\")\n",
    "\n",
    "\n",
    "def load_icarl_checkpoint(icarl_instance, checkpoint_dir):\n",
    "    \"\"\"\n",
    "    Loads the latest checkpoint from the directory and restores the iCaRL state.\n",
    "    Returns: The task_id to resume from (e.g., if task 2 was the last saved, returns 3).\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Find the latest/best checkpoint file\n",
    "    checkpoint_files = [\n",
    "        f\n",
    "        for f in os.listdir(checkpoint_dir)\n",
    "        if f.startswith(\"icarl_task\") and f.endswith(\".pth\")\n",
    "    ]\n",
    "    if not checkpoint_files:\n",
    "        print(\" No checkpoint found. Starting from Task 1.\")\n",
    "        return 0  # Start from task 0 (which becomes task 1 in the loop)\n",
    "\n",
    "    # Simple heuristic: pick the one with the highest task ID in the filename\n",
    "    latest_file = max(checkpoint_files, key=lambda f: int(f.split(\"_\")[2]))\n",
    "    load_path = os.path.join(checkpoint_dir, latest_file)\n",
    "    print(f\"Loading checkpoint from: {load_path}\")\n",
    "\n",
    "    # 2. Load the state\n",
    "    checkpoint = torch.load(load_path, map_location=icarl_instance.device)\n",
    "\n",
    "    # 3. Restore iCaRL state\n",
    "\n",
    "    # Restore Model\n",
    "    icarl_instance.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "    # Restore Old Model\n",
    "    old_state_dict = checkpoint[\"old_model_state_dict\"]\n",
    "    if old_state_dict:\n",
    "        # We need a new model instance to load the old state into\n",
    "        icarl_instance.old_model = CustomDino(\n",
    "            num_classes=icarl_instance.num_classes\n",
    "        ).to(icarl_instance.device)\n",
    "        icarl_instance.old_model.load_state_dict(old_state_dict)\n",
    "        for p in icarl_instance.old_model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    # Restore Exemplar Sets (move back to GPU if necessary)\n",
    "    # The images in exemplar sets are Tensors\n",
    "    icarl_instance.exemplar_sets = [\n",
    "        [img.to(icarl_instance.device) for img in class_set]\n",
    "        for class_set in checkpoint[\"exemplar_sets\"]\n",
    "    ]\n",
    "\n",
    "    # Determine next task\n",
    "    last_completed_task = checkpoint[\"task_id\"]\n",
    "    print(\n",
    "        f\"Resuming from after Task {last_completed_task} (Acc: {checkpoint['accuracy']:.2f}%).\"\n",
    "    )\n",
    "    return last_completed_task + 1\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 4. Data Utilities\n",
    "# ==========================================\n",
    "class iCaRLDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset that combines new task data with stored exemplars.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, new_data, exemplars, transform=None):\n",
    "        self.new_data = new_data  # List of (image, label) tuples\n",
    "        self.exemplars = exemplars  # List of lists of images\n",
    "        self.transform = transform\n",
    "\n",
    "        # Flatten exemplars into a list of (img, label)\n",
    "        self.exemplar_data = []\n",
    "        for label, img_list in enumerate(exemplars):\n",
    "            for img in img_list:\n",
    "                self.exemplar_data.append((img, label))\n",
    "\n",
    "        self.all_data = self.new_data + self.exemplar_data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, label = self.all_data[index]\n",
    "        # img is a Tensor here if coming from CIFAR100(ToTensor),\n",
    "        # but iCaRL usually stores raw images.\n",
    "        # For simplicity in this script, we assume img is already Tensor from prev loader\n",
    "        # If transform is needed, apply here.\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_data)\n",
    "\n",
    "\n",
    "def get_data_for_classes(dataset, classes):\n",
    "    \"\"\"\n",
    "    Extracts all samples belonging to specific classes.\n",
    "    \"\"\"\n",
    "    indices = [i for i, label in enumerate(dataset.targets) if label in classes]\n",
    "    return Subset(dataset, indices)\n",
    "\n",
    "\n",
    "def extract_images_from_subset(subset):\n",
    "    \"\"\"\n",
    "    Helper to pull images out of a Subset for exemplar storage.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    # This is slow for large sets, efficient implementation would use indices directly\n",
    "    # But for a tutorial script, iterating is safe.\n",
    "    for i in range(len(subset)):\n",
    "        img, _ = subset[i]\n",
    "        images.append(img)\n",
    "    return images\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 5. Main Experiment Loop (single round over all 100 classes)\n",
    "# ==========================================\n",
    "def main():\n",
    "    print(\"Preparing Data...\")\n",
    "    stats = ((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
    "    transform = Compose(\n",
    "        [\n",
    "            Resize(256),\n",
    "            CenterCrop(224),\n",
    "            ToTensor(),\n",
    "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    run = wandb.init(\n",
    "        entity=WANDB_ENTITY,\n",
    "        project=WANDB_PROJECT,\n",
    "        group=WANDB_GROUP,\n",
    "        name=WANDB_RUN_NAME,\n",
    "        notes=WANDB_NOTES,\n",
    "        mode=WANDB_MODE,\n",
    "    )\n",
    "\n",
    "    # Load full CIFAR-100\n",
    "    train_ds = datasets.CIFAR100(\n",
    "        root=\"./data\", train=True, download=True, transform=transform\n",
    "    )\n",
    "\n",
    "    # Model (no incremental steps)\n",
    "    model = CustomDino(num_classes=100).to(DEVICE)\n",
    "\n",
    "    # Compute centroids using TOTAL_EXEMPLARS_VECTORS / 100 images per class\n",
    "    images_per_class = TOTAL_EXEMPLARS_VECTORS // 100\n",
    "    print(f\"Using {images_per_class} images per class for centroid computation\")\n",
    "\n",
    "    model.eval()\n",
    "    centroids_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for class_id in tqdm(range(100), desc=\"Computing centroids per class\"):\n",
    "            # Get indices for this class\n",
    "            class_indices = [\n",
    "                i for i, label in enumerate(train_ds.targets) if label == class_id\n",
    "            ]\n",
    "\n",
    "            # Select only images_per_class images\n",
    "            selected_indices = class_indices[:images_per_class]\n",
    "\n",
    "            # Extract features for selected images\n",
    "            class_features = []\n",
    "            for idx in selected_indices:\n",
    "                img, _ = train_ds[idx]\n",
    "                img = img.unsqueeze(0).to(DEVICE)\n",
    "                _, feat = model(img)\n",
    "                class_features.append(feat)\n",
    "\n",
    "            # Compute mean of features, then normalize (DINO style)\n",
    "            if class_features:\n",
    "                class_mean = torch.mean(\n",
    "                    torch.cat(class_features, dim=0), dim=0, keepdim=True\n",
    "                )\n",
    "                class_centroid = F.normalize(class_mean, p=2, dim=1).squeeze(0)\n",
    "                centroids_list.append(class_centroid)\n",
    "            else:\n",
    "                centroids_list.append(torch.zeros(384, device=DEVICE))\n",
    "\n",
    "    centroids = torch.stack(centroids_list).cpu()\n",
    "\n",
    "    # Save centroids as a Linear state_dict-compatible payload\n",
    "    state_dict = {\n",
    "        \"weight\": centroids,\n",
    "        \"bias\": torch.zeros(100),\n",
    "    }\n",
    "    torch.save(state_dict, FILENAME)\n",
    "    print(f\"Saved centroids as linear weights to {FILENAME}\")\n",
    "\n",
    "    save_checkpoint_to_wandb(\n",
    "        run=run,\n",
    "        checkpoint=state_dict,\n",
    "        filename=FILENAME,\n",
    "        metadata={\n",
    "            \"type\": \"centroids\",\n",
    "            \"num_classes\": 100,\n",
    "            \"feature_dim\": 384,\n",
    "            \"images_per_class\": images_per_class,\n",
    "        },\n",
    "    )\n",
    "    run.finish()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/einrich99/Progetti/FL-task-arithmetic/notebooks/wandb/run-20251211_193452-fvj6s29t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aml-fl-project/fl-task-arithmetic/runs/fvj6s29t' target=\"_blank\">icarl-centroids-eval</a></strong> to <a href='https://wandb.ai/aml-fl-project/fl-task-arithmetic' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aml-fl-project/fl-task-arithmetic' target=\"_blank\">https://wandb.ai/aml-fl-project/fl-task-arithmetic</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aml-fl-project/fl-task-arithmetic/runs/fvj6s29t' target=\"_blank\">https://wandb.ai/aml-fl-project/fl-task-arithmetic/runs/fvj6s29t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/einrich99/.cache/torch/hub/facebookresearch_dino_main\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/einrich99/Progetti/FL-task-arithmetic/notebooks/artifacts/centroids-checkpoints:v3/centroids_linear.pth\n",
      "Successfully loaded model from: /home/einrich99/Progetti/FL-task-arithmetic/notebooks/artifacts/centroids-checkpoints:v3/centroids_linear.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 157/157 [06:22<00:00,  2.43s/it]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with WandB centroids: 0.5149\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td>0.5149</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">icarl-centroids-eval</strong> at: <a href='https://wandb.ai/aml-fl-project/fl-task-arithmetic/runs/fvj6s29t' target=\"_blank\">https://wandb.ai/aml-fl-project/fl-task-arithmetic/runs/fvj6s29t</a><br> View project at: <a href='https://wandb.ai/aml-fl-project/fl-task-arithmetic' target=\"_blank\">https://wandb.ai/aml-fl-project/fl-task-arithmetic</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251211_193452-fvj6s29t/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load centroids from WandB and evaluate on CIFAR-100 test\n",
    "from utilities.wandb_utils import load_checkpoint_from_wandb\n",
    "import wandb\n",
    "\n",
    "# Init run to reuse artifact\n",
    "run = wandb.init(\n",
    "    entity=WANDB_ENTITY,\n",
    "    project=WANDB_PROJECT,\n",
    "    group=WANDB_GROUP,\n",
    "    name=f\"{WANDB_RUN_NAME}-eval\",\n",
    "    notes=\"Eval centroids on CIFAR100 test\",\n",
    "    mode=WANDB_MODE,\n",
    ")\n",
    "\n",
    "# Prepare model with pretrained backbone\n",
    "model = CustomDino(num_classes=100).to(DEVICE)\n",
    "\n",
    "# Download checkpoint artifact - it returns a dict with 'weight' and 'bias' keys\n",
    "result = load_checkpoint_from_wandb(\n",
    "    run=run,\n",
    "    model=model,  # Needed for device detection\n",
    "    filename=FILENAME,\n",
    "    version=\"latest\",\n",
    ")\n",
    "\n",
    "if result is None:\n",
    "    print(\"No checkpoint found on WandB; aborting eval.\")\n",
    "    run.finish()\n",
    "else:\n",
    "    checkpoint, _artifact = result\n",
    "    # Load the centroids into the classifier layer\n",
    "    model.classifier.load_state_dict(checkpoint)\n",
    "    model.eval()\n",
    "\n",
    "    # CIFAR-100 test loader\n",
    "    stats = ((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
    "    transform = Compose(\n",
    "        [\n",
    "            Resize(256),\n",
    "            CenterCrop(224),\n",
    "            ToTensor(),\n",
    "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "    test_ds = datasets.CIFAR100(\n",
    "        root=\"./data\", train=False, download=True, transform=transform\n",
    "    )\n",
    "    test_loader = DataLoader(test_ds, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "    # Evaluate accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            logits, _ = model(imgs)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    acc = correct / max(total, 1)\n",
    "    print(f\"Test accuracy with WandB centroids: {acc:.4f}\")\n",
    "    run.log({\"test_accuracy\": acc})\n",
    "    run.finish()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml_flower",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
