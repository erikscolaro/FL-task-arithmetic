# Sparse Fine-tuning Configuration for FL-task-arithmetic
# This file contains example configurations for different sparse fine-tuning experiments

[experiment-baseline]
# Baseline: No sparsity, standard fine-tuning with CustomDino
use-sparse-finetuning = false
use-custom-dino = true
num-server-rounds = 10
local-epochs = 3
lr = 0.001
sparsity-ratio = 0.0
num-calibration-rounds = 1
num-batches-calibration = 10

[experiment-sparse-50]
# 50% sparsity - moderate parameter freezing
use-sparse-finetuning = true
use-custom-dino = true
num-server-rounds = 10
local-epochs = 3
lr = 0.001
sparsity-ratio = 0.5
num-calibration-rounds = 3
num-batches-calibration = 10

[experiment-sparse-70]
# 70% sparsity - aggressive parameter freezing
use-sparse-finetuning = true
use-custom-dino = true
num-server-rounds = 10
local-epochs = 3
lr = 0.001
sparsity-ratio = 0.7
num-calibration-rounds = 3
num-batches-calibration = 10

[experiment-sparse-90]
# 90% sparsity - extreme parameter freezing
use-sparse-finetuning = true
use-custom-dino = true
num-server-rounds = 10
local-epochs = 3
lr = 0.001
sparsity-ratio = 0.9
num-calibration-rounds = 5
num-batches-calibration = 10

[experiment-calibration-rounds]
# Test effect of calibration rounds
use-sparse-finetuning = true
use-custom-dino = true
num-server-rounds = 10
local-epochs = 3
lr = 0.001
sparsity-ratio = 0.7
num-calibration-rounds = 10
num-batches-calibration = 10

# How to use these configurations:
# 
# 1. Copy the desired experiment section to pyproject.toml [tool.flwr.app.config]
# 2. Or override specific parameters when running:
#    flwr run . --run-config use-sparse-finetuning=true sparsity-ratio=0.7
# 
# 3. For grid search experiments, create a sweep configuration for wandb:
#    See wandb_sweep_config.yaml for hyperparameter optimization
