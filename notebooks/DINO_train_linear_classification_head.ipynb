{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e94738e6",
   "metadata": {},
   "source": [
    "# Precompute features and associated labels for the CIFAR 100 train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "138e2903",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/einrich99/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10/1563 (0.6%) completed\n",
      "Batch 20/1563 (1.3%) completed\n",
      "Batch 30/1563 (1.9%) completed\n",
      "Batch 40/1563 (2.6%) completed\n",
      "Batch 50/1563 (3.2%) completed\n",
      "Batch 60/1563 (3.8%) completed\n",
      "Batch 70/1563 (4.5%) completed\n",
      "Batch 80/1563 (5.1%) completed\n",
      "Batch 90/1563 (5.8%) completed\n",
      "Batch 100/1563 (6.4%) completed\n",
      "Batch 110/1563 (7.0%) completed\n",
      "Batch 120/1563 (7.7%) completed\n",
      "Batch 130/1563 (8.3%) completed\n",
      "Batch 140/1563 (9.0%) completed\n",
      "Batch 150/1563 (9.6%) completed\n",
      "Batch 160/1563 (10.2%) completed\n",
      "Batch 170/1563 (10.9%) completed\n",
      "Batch 180/1563 (11.5%) completed\n",
      "Batch 190/1563 (12.2%) completed\n",
      "Batch 200/1563 (12.8%) completed\n",
      "Batch 210/1563 (13.4%) completed\n",
      "Batch 220/1563 (14.1%) completed\n",
      "Batch 230/1563 (14.7%) completed\n",
      "Batch 240/1563 (15.4%) completed\n",
      "Batch 250/1563 (16.0%) completed\n",
      "Batch 260/1563 (16.6%) completed\n",
      "Batch 270/1563 (17.3%) completed\n",
      "Batch 280/1563 (17.9%) completed\n",
      "Batch 290/1563 (18.6%) completed\n",
      "Batch 300/1563 (19.2%) completed\n",
      "Batch 310/1563 (19.8%) completed\n",
      "Batch 320/1563 (20.5%) completed\n",
      "Batch 330/1563 (21.1%) completed\n",
      "Batch 340/1563 (21.8%) completed\n",
      "Batch 350/1563 (22.4%) completed\n",
      "Batch 360/1563 (23.0%) completed\n",
      "Batch 370/1563 (23.7%) completed\n",
      "Batch 380/1563 (24.3%) completed\n",
      "Batch 390/1563 (25.0%) completed\n",
      "Batch 400/1563 (25.6%) completed\n",
      "Batch 410/1563 (26.2%) completed\n",
      "Batch 420/1563 (26.9%) completed\n",
      "Batch 430/1563 (27.5%) completed\n",
      "Batch 440/1563 (28.2%) completed\n",
      "Batch 450/1563 (28.8%) completed\n",
      "Batch 460/1563 (29.4%) completed\n",
      "Batch 470/1563 (30.1%) completed\n",
      "Batch 480/1563 (30.7%) completed\n",
      "Batch 490/1563 (31.3%) completed\n",
      "Batch 500/1563 (32.0%) completed\n",
      "Batch 510/1563 (32.6%) completed\n",
      "Batch 520/1563 (33.3%) completed\n",
      "Batch 530/1563 (33.9%) completed\n",
      "Batch 540/1563 (34.5%) completed\n",
      "Batch 550/1563 (35.2%) completed\n",
      "Batch 560/1563 (35.8%) completed\n",
      "Batch 570/1563 (36.5%) completed\n",
      "Batch 580/1563 (37.1%) completed\n",
      "Batch 590/1563 (37.7%) completed\n",
      "Batch 600/1563 (38.4%) completed\n",
      "Batch 610/1563 (39.0%) completed\n",
      "Batch 620/1563 (39.7%) completed\n",
      "Batch 630/1563 (40.3%) completed\n",
      "Batch 640/1563 (40.9%) completed\n",
      "Batch 650/1563 (41.6%) completed\n",
      "Batch 660/1563 (42.2%) completed\n",
      "Batch 670/1563 (42.9%) completed\n",
      "Batch 680/1563 (43.5%) completed\n",
      "Batch 690/1563 (44.1%) completed\n",
      "Batch 700/1563 (44.8%) completed\n",
      "Batch 710/1563 (45.4%) completed\n",
      "Batch 720/1563 (46.1%) completed\n",
      "Batch 730/1563 (46.7%) completed\n",
      "Batch 740/1563 (47.3%) completed\n",
      "Batch 750/1563 (48.0%) completed\n",
      "Batch 760/1563 (48.6%) completed\n",
      "Batch 770/1563 (49.3%) completed\n",
      "Batch 780/1563 (49.9%) completed\n",
      "Batch 790/1563 (50.5%) completed\n",
      "Batch 800/1563 (51.2%) completed\n",
      "Batch 810/1563 (51.8%) completed\n",
      "Batch 820/1563 (52.5%) completed\n",
      "Batch 830/1563 (53.1%) completed\n",
      "Batch 840/1563 (53.7%) completed\n",
      "Batch 850/1563 (54.4%) completed\n",
      "Batch 860/1563 (55.0%) completed\n",
      "Batch 870/1563 (55.7%) completed\n",
      "Batch 880/1563 (56.3%) completed\n",
      "Batch 890/1563 (56.9%) completed\n",
      "Batch 900/1563 (57.6%) completed\n",
      "Batch 910/1563 (58.2%) completed\n",
      "Batch 920/1563 (58.9%) completed\n",
      "Batch 930/1563 (59.5%) completed\n",
      "Batch 940/1563 (60.1%) completed\n",
      "Batch 950/1563 (60.8%) completed\n",
      "Batch 960/1563 (61.4%) completed\n",
      "Batch 970/1563 (62.1%) completed\n",
      "Batch 980/1563 (62.7%) completed\n",
      "Batch 990/1563 (63.3%) completed\n",
      "Batch 1000/1563 (64.0%) completed\n",
      "Batch 1010/1563 (64.6%) completed\n",
      "Batch 1020/1563 (65.3%) completed\n",
      "Batch 1030/1563 (65.9%) completed\n",
      "Batch 1040/1563 (66.5%) completed\n",
      "Batch 1050/1563 (67.2%) completed\n",
      "Batch 1060/1563 (67.8%) completed\n",
      "Batch 1070/1563 (68.5%) completed\n",
      "Batch 1080/1563 (69.1%) completed\n",
      "Batch 1090/1563 (69.7%) completed\n",
      "Batch 1100/1563 (70.4%) completed\n",
      "Batch 1110/1563 (71.0%) completed\n",
      "Batch 1120/1563 (71.7%) completed\n",
      "Batch 1130/1563 (72.3%) completed\n",
      "Batch 1140/1563 (72.9%) completed\n",
      "Batch 1150/1563 (73.6%) completed\n",
      "Batch 1160/1563 (74.2%) completed\n",
      "Batch 1170/1563 (74.9%) completed\n",
      "Batch 1180/1563 (75.5%) completed\n",
      "Batch 1190/1563 (76.1%) completed\n",
      "Batch 1200/1563 (76.8%) completed\n",
      "Batch 1210/1563 (77.4%) completed\n",
      "Batch 1220/1563 (78.1%) completed\n",
      "Batch 1230/1563 (78.7%) completed\n",
      "Batch 1240/1563 (79.3%) completed\n",
      "Batch 1250/1563 (80.0%) completed\n",
      "Batch 1260/1563 (80.6%) completed\n",
      "Batch 1270/1563 (81.3%) completed\n",
      "Batch 1280/1563 (81.9%) completed\n",
      "Batch 1290/1563 (82.5%) completed\n",
      "Batch 1300/1563 (83.2%) completed\n",
      "Batch 1310/1563 (83.8%) completed\n",
      "Batch 1320/1563 (84.5%) completed\n",
      "Batch 1330/1563 (85.1%) completed\n",
      "Batch 1340/1563 (85.7%) completed\n",
      "Batch 1350/1563 (86.4%) completed\n",
      "Batch 1360/1563 (87.0%) completed\n",
      "Batch 1370/1563 (87.7%) completed\n",
      "Batch 1380/1563 (88.3%) completed\n",
      "Batch 1390/1563 (88.9%) completed\n",
      "Batch 1400/1563 (89.6%) completed\n",
      "Batch 1410/1563 (90.2%) completed\n",
      "Batch 1420/1563 (90.9%) completed\n",
      "Batch 1430/1563 (91.5%) completed\n",
      "Batch 1440/1563 (92.1%) completed\n",
      "Batch 1450/1563 (92.8%) completed\n",
      "Batch 1460/1563 (93.4%) completed\n",
      "Batch 1470/1563 (94.0%) completed\n",
      "Batch 1480/1563 (94.7%) completed\n",
      "Batch 1490/1563 (95.3%) completed\n",
      "Batch 1500/1563 (96.0%) completed\n",
      "Batch 1510/1563 (96.6%) completed\n",
      "Batch 1520/1563 (97.2%) completed\n",
      "Batch 1530/1563 (97.9%) completed\n",
      "Batch 1540/1563 (98.5%) completed\n",
      "Batch 1550/1563 (99.2%) completed\n",
      "Batch 1560/1563 (99.8%) completed\n",
      "Batch 1563/1563 (100.0%) completed\n",
      "Batch 10/313 (3.2%) completed\n",
      "Batch 20/313 (6.4%) completed\n",
      "Batch 30/313 (9.6%) completed\n",
      "Batch 40/313 (12.8%) completed\n",
      "Batch 50/313 (16.0%) completed\n",
      "Batch 60/313 (19.2%) completed\n",
      "Batch 70/313 (22.4%) completed\n",
      "Batch 80/313 (25.6%) completed\n",
      "Batch 90/313 (28.8%) completed\n",
      "Batch 100/313 (31.9%) completed\n",
      "Batch 110/313 (35.1%) completed\n",
      "Batch 120/313 (38.3%) completed\n",
      "Batch 130/313 (41.5%) completed\n",
      "Batch 140/313 (44.7%) completed\n",
      "Batch 150/313 (47.9%) completed\n",
      "Batch 160/313 (51.1%) completed\n",
      "Batch 170/313 (54.3%) completed\n",
      "Batch 180/313 (57.5%) completed\n",
      "Batch 190/313 (60.7%) completed\n",
      "Batch 200/313 (63.9%) completed\n",
      "Batch 210/313 (67.1%) completed\n",
      "Batch 220/313 (70.3%) completed\n",
      "Batch 230/313 (73.5%) completed\n",
      "Batch 240/313 (76.7%) completed\n",
      "Batch 250/313 (79.9%) completed\n",
      "Batch 260/313 (83.1%) completed\n",
      "Batch 270/313 (86.3%) completed\n",
      "Batch 280/313 (89.5%) completed\n",
      "Batch 290/313 (92.7%) completed\n",
      "Batch 300/313 (95.8%) completed\n",
      "Batch 310/313 (99.0%) completed\n",
      "Batch 313/313 (100.0%) completed\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import cast\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR100\n",
    "\n",
    "# Load DINO ViT-S/16 pre-trained from torch.hub\n",
    "\n",
    "dino_model = cast(\n",
    "    nn.Module,\n",
    "    torch.hub.load(\"facebookresearch/dino:main\", \"dino_vits16\", pretrained=True),\n",
    ")\n",
    "dino_model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dino_model.to(device=device)\n",
    "\n",
    "# Use the preprocess defined in the previous cell\n",
    "# Make sure the dataset uses the correct preprocess\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        # in federated training, we should consider to use mean and std of the cifar100\n",
    "        # these are the parameters on which dino was trained\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = CIFAR100(root=\"./data\", train=True, download=True, transform=preprocess)\n",
    "test_dataset = CIFAR100(root=\"./data\", train=False, download=True, transform=preprocess)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "# Function to extract features from a dataloader\n",
    "def extract_features_and_labels(dataloader, model, device):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        total_batches = len(dataloader)\n",
    "        for batch_idx, (images, labels) in enumerate(dataloader):\n",
    "            images = images.to(device)\n",
    "            # Get features from the backbone (without the classification head)\n",
    "            features = model(images)\n",
    "            all_features.append(features.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "            if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == total_batches:\n",
    "                print(\n",
    "                    f\"Batch {batch_idx + 1}/{total_batches} ({(batch_idx + 1) / total_batches:.1%}) completed\"\n",
    "                )\n",
    "    all_features = torch.cat(all_features, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    return all_features, all_labels\n",
    "\n",
    "\n",
    "# Extract features and labels for train\n",
    "train_features, train_labels = extract_features_and_labels(\n",
    "    train_loader, dino_model, device\n",
    ")\n",
    "torch.save(\n",
    "    {\"features\": train_features, \"labels\": train_labels},\n",
    "    \"features/train_features.pt\",\n",
    ")\n",
    "\n",
    "# Extract features and labels for test\n",
    "test_features, test_labels = extract_features_and_labels(\n",
    "    test_loader, dino_model, device\n",
    ")\n",
    "torch.save(\n",
    "    {\"features\": test_features, \"labels\": test_labels}, \"features/test_features.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2851731a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/einrich99/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb7a36822dcf4b109984dd2f948cb1bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 Training:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e2c789e98d64657b94752c46a204712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 Evaluation:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2af65e70d02347879886d31253d30199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 Training:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b56fa35d3644c5a28adee47d51afc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 Evaluation:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2339d06537634559a6aecf3625b4bc7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3 Training:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0457c468884344fd80ab7c6775c7be7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3 Evaluation:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85dc4c4f49e249de88777eb8c9ad9931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4 Training:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "599c394c70d64fac8c33449a02245e4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4 Evaluation:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75c2fa2ade894c30b85a39f5c583ad64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5 Training:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d0416f2e0ea475289bd2594f66d77dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5 Evaluation:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4c7eed47e2a489094f3fbb8fb712be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6 Training:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c93e36ce1d0d44fab09d2d48d8c98ff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6 Evaluation:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24470ee20cd34ae48e0f684c9ba2f2ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7 Training:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64abbc79de1c432ba7fca3056b340bff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7 Evaluation:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb11279d5f954f7e828430a6ac0e9e99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8 Training:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d91e5d38e554388bcf86dbbbccd4a10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8 Evaluation:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "889b40e3ab6a499792d31a94665ac260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9 Training:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7c48c38f2684e33b0ac3614b3a4a3e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9 Evaluation:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24e0a50eb07d432d9c332e80d0b6fd0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10 Training:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ce0a0cc33fe4c9b891f0b68b3486d51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10 Evaluation:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc2f7ceda504b1498386cf65f8bd1fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11 Training:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d620796a1d4b4293a9a9ea124bd99e41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11 Evaluation:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional, cast\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 100\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Early stopping parameters\n",
    "best_acc = 0\n",
    "patience_counter = 0\n",
    "best_model_state = {}\n",
    "patience = 0\n",
    "\n",
    "dino_pretrained = cast(\n",
    "    nn.Module,\n",
    "    torch.hub.load(\"facebookresearch/dino:main\", \"dino_vits16\", pretrained=True),\n",
    ")\n",
    "\n",
    "\n",
    "class CustomDino(nn.Module):\n",
    "    def __init__(self, num_classes: int = 100, backbone: Optional[nn.Module] = None):\n",
    "        super().__init__()\n",
    "        if backbone is None:\n",
    "            # Carica DINO senza pretrained e rimuove la head\n",
    "            backbone = cast(\n",
    "                nn.Module,\n",
    "                torch.hub.load(\n",
    "                    \"facebookresearch/dino:main\", \"dino_vits16\", pretrained=False\n",
    "                ),\n",
    "            )\n",
    "        self.backbone: nn.Module = backbone\n",
    "        self.classifier = nn.Linear(\n",
    "            384, num_classes\n",
    "        )  # 384 = output CLS token DINO ViT-S/16\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        features = self.backbone(x)  # [batch, 384]\n",
    "        logits = self.classifier(features)  # [batch, num_classes]\n",
    "        return logits  # , features\n",
    "\n",
    "\n",
    "model = CustomDino(num_classes=100, backbone=dino_pretrained)\n",
    "\n",
    "# Example preprocessing for an input image\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        # in federated training, we should consider to use mean and std of the cifar100\n",
    "        # these are the parameters on which dino was trained\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Load precomputed features and labels\n",
    "train_data = torch.load(\"features/train_features.pt\")\n",
    "test_data = torch.load(\"features/test_features.pt\")\n",
    "\n",
    "train_features, train_labels = train_data[\"features\"], train_data[\"labels\"]\n",
    "test_features, test_labels = test_data[\"features\"], test_data[\"labels\"]\n",
    "\n",
    "# Create TensorDatasets and DataLoaders from features\n",
    "train_dataset = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Optimizer for the head\n",
    "optimizer = torch.optim.Adam(model.classifier.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -----------------------------------\n",
    "# Train only the linear head for fun startin from the features\n",
    "# -----------------------------------\n",
    "\n",
    "complete_model = model\n",
    "model = model.classifier\n",
    "model.to(device=device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Create a epoch-level progress bar and update it per-batch\n",
    "    epoch_desc = f\"Epoch {epoch+1} Training\"\n",
    "    with tqdm(total=len(train_loader), desc=epoch_desc, leave=True) as progress:\n",
    "        for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "            # Move tensors to device\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * features.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            # Update the progress bar with running metrics\n",
    "            batch_loss = running_loss / total if total > 0 else 0\n",
    "            batch_acc = correct / total if total > 0 else 0\n",
    "            progress.set_postfix(\n",
    "                {\"loss\": f\"{batch_loss:.4f}\", \"acc\": f\"{batch_acc:.4f}\"}\n",
    "            )\n",
    "            progress.update(1)\n",
    "\n",
    "    epoch_loss = running_loss / total if total > 0 else 0\n",
    "    epoch_acc = correct / total if total > 0 else 0\n",
    "\n",
    "    # Evaluation on test set\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_acc = 0.0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "\n",
    "    # Create a per-epoch evaluation progress bar\n",
    "    eval_desc = f\"Epoch {epoch+1} Evaluation\"\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(test_loader), desc=eval_desc, leave=True) as test_progress:\n",
    "            for features, labels in test_loader:\n",
    "                features, labels = features.to(device), labels.to(device)\n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item() * features.size(0)\n",
    "                _, predicted = outputs.max(1)\n",
    "                test_total += labels.size(0)\n",
    "                test_correct += predicted.eq(labels).sum().item()\n",
    "                test_progress.update(1)\n",
    "\n",
    "            test_loss = test_loss / test_total if test_total > 0 else 0\n",
    "            test_acc = test_correct / test_total if test_total > 0 else 0\n",
    "\n",
    "            test_progress.set_postfix(\n",
    "                {\n",
    "                    \"loss\": f\"{test_loss / test_total if test_total > 0 else 0:.4f}\",\n",
    "                    \"acc\": f\"{test_acc:.4f}\",\n",
    "                }\n",
    "            )\n",
    "            test_progress.update(1)\n",
    "\n",
    "    # Early stopping\n",
    "    if epoch == 0:\n",
    "        best_acc = test_acc\n",
    "        patience = 3\n",
    "        patience_counter = 0\n",
    "        best_model_state = model.state_dict()\n",
    "    else:\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                model.load_state_dict(best_model_state)\n",
    "                break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_flower",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
