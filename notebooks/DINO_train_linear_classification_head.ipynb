{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e94738e6",
   "metadata": {},
   "source": [
    "# Precompute features and associated labels for the CIFAR 100 train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "138e2903",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/einrich99/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10/1563 (0.6%) completed\n",
      "Batch 20/1563 (1.3%) completed\n",
      "Batch 30/1563 (1.9%) completed\n",
      "Batch 40/1563 (2.6%) completed\n",
      "Batch 50/1563 (3.2%) completed\n",
      "Batch 60/1563 (3.8%) completed\n",
      "Batch 70/1563 (4.5%) completed\n",
      "Batch 80/1563 (5.1%) completed\n",
      "Batch 90/1563 (5.8%) completed\n",
      "Batch 100/1563 (6.4%) completed\n",
      "Batch 110/1563 (7.0%) completed\n",
      "Batch 120/1563 (7.7%) completed\n",
      "Batch 130/1563 (8.3%) completed\n",
      "Batch 140/1563 (9.0%) completed\n",
      "Batch 150/1563 (9.6%) completed\n",
      "Batch 160/1563 (10.2%) completed\n",
      "Batch 170/1563 (10.9%) completed\n",
      "Batch 180/1563 (11.5%) completed\n",
      "Batch 190/1563 (12.2%) completed\n",
      "Batch 200/1563 (12.8%) completed\n",
      "Batch 210/1563 (13.4%) completed\n",
      "Batch 220/1563 (14.1%) completed\n",
      "Batch 230/1563 (14.7%) completed\n",
      "Batch 240/1563 (15.4%) completed\n",
      "Batch 250/1563 (16.0%) completed\n",
      "Batch 260/1563 (16.6%) completed\n",
      "Batch 270/1563 (17.3%) completed\n",
      "Batch 280/1563 (17.9%) completed\n",
      "Batch 290/1563 (18.6%) completed\n",
      "Batch 300/1563 (19.2%) completed\n",
      "Batch 310/1563 (19.8%) completed\n",
      "Batch 320/1563 (20.5%) completed\n",
      "Batch 330/1563 (21.1%) completed\n",
      "Batch 340/1563 (21.8%) completed\n",
      "Batch 350/1563 (22.4%) completed\n",
      "Batch 360/1563 (23.0%) completed\n",
      "Batch 370/1563 (23.7%) completed\n",
      "Batch 380/1563 (24.3%) completed\n",
      "Batch 390/1563 (25.0%) completed\n",
      "Batch 400/1563 (25.6%) completed\n",
      "Batch 410/1563 (26.2%) completed\n",
      "Batch 420/1563 (26.9%) completed\n",
      "Batch 430/1563 (27.5%) completed\n",
      "Batch 440/1563 (28.2%) completed\n",
      "Batch 450/1563 (28.8%) completed\n",
      "Batch 460/1563 (29.4%) completed\n",
      "Batch 470/1563 (30.1%) completed\n",
      "Batch 480/1563 (30.7%) completed\n",
      "Batch 490/1563 (31.3%) completed\n",
      "Batch 500/1563 (32.0%) completed\n",
      "Batch 510/1563 (32.6%) completed\n",
      "Batch 520/1563 (33.3%) completed\n",
      "Batch 530/1563 (33.9%) completed\n",
      "Batch 540/1563 (34.5%) completed\n",
      "Batch 550/1563 (35.2%) completed\n",
      "Batch 560/1563 (35.8%) completed\n",
      "Batch 570/1563 (36.5%) completed\n",
      "Batch 580/1563 (37.1%) completed\n",
      "Batch 590/1563 (37.7%) completed\n",
      "Batch 600/1563 (38.4%) completed\n",
      "Batch 610/1563 (39.0%) completed\n",
      "Batch 620/1563 (39.7%) completed\n",
      "Batch 630/1563 (40.3%) completed\n",
      "Batch 640/1563 (40.9%) completed\n",
      "Batch 650/1563 (41.6%) completed\n",
      "Batch 660/1563 (42.2%) completed\n",
      "Batch 670/1563 (42.9%) completed\n",
      "Batch 680/1563 (43.5%) completed\n",
      "Batch 690/1563 (44.1%) completed\n",
      "Batch 700/1563 (44.8%) completed\n",
      "Batch 710/1563 (45.4%) completed\n",
      "Batch 720/1563 (46.1%) completed\n",
      "Batch 730/1563 (46.7%) completed\n",
      "Batch 740/1563 (47.3%) completed\n",
      "Batch 750/1563 (48.0%) completed\n",
      "Batch 760/1563 (48.6%) completed\n",
      "Batch 770/1563 (49.3%) completed\n",
      "Batch 780/1563 (49.9%) completed\n",
      "Batch 790/1563 (50.5%) completed\n",
      "Batch 800/1563 (51.2%) completed\n",
      "Batch 810/1563 (51.8%) completed\n",
      "Batch 820/1563 (52.5%) completed\n",
      "Batch 830/1563 (53.1%) completed\n",
      "Batch 840/1563 (53.7%) completed\n",
      "Batch 850/1563 (54.4%) completed\n",
      "Batch 860/1563 (55.0%) completed\n",
      "Batch 870/1563 (55.7%) completed\n",
      "Batch 880/1563 (56.3%) completed\n",
      "Batch 890/1563 (56.9%) completed\n",
      "Batch 900/1563 (57.6%) completed\n",
      "Batch 910/1563 (58.2%) completed\n",
      "Batch 920/1563 (58.9%) completed\n",
      "Batch 930/1563 (59.5%) completed\n",
      "Batch 940/1563 (60.1%) completed\n",
      "Batch 950/1563 (60.8%) completed\n",
      "Batch 960/1563 (61.4%) completed\n",
      "Batch 970/1563 (62.1%) completed\n",
      "Batch 980/1563 (62.7%) completed\n",
      "Batch 990/1563 (63.3%) completed\n",
      "Batch 1000/1563 (64.0%) completed\n",
      "Batch 1010/1563 (64.6%) completed\n",
      "Batch 1020/1563 (65.3%) completed\n",
      "Batch 1030/1563 (65.9%) completed\n",
      "Batch 1040/1563 (66.5%) completed\n",
      "Batch 1050/1563 (67.2%) completed\n",
      "Batch 1060/1563 (67.8%) completed\n",
      "Batch 1070/1563 (68.5%) completed\n",
      "Batch 1080/1563 (69.1%) completed\n",
      "Batch 1090/1563 (69.7%) completed\n",
      "Batch 1100/1563 (70.4%) completed\n",
      "Batch 1110/1563 (71.0%) completed\n",
      "Batch 1120/1563 (71.7%) completed\n",
      "Batch 1130/1563 (72.3%) completed\n",
      "Batch 1140/1563 (72.9%) completed\n",
      "Batch 1150/1563 (73.6%) completed\n",
      "Batch 1160/1563 (74.2%) completed\n",
      "Batch 1170/1563 (74.9%) completed\n",
      "Batch 1180/1563 (75.5%) completed\n",
      "Batch 1190/1563 (76.1%) completed\n",
      "Batch 1200/1563 (76.8%) completed\n",
      "Batch 1210/1563 (77.4%) completed\n",
      "Batch 1220/1563 (78.1%) completed\n",
      "Batch 1230/1563 (78.7%) completed\n",
      "Batch 1240/1563 (79.3%) completed\n",
      "Batch 1250/1563 (80.0%) completed\n",
      "Batch 1260/1563 (80.6%) completed\n",
      "Batch 1270/1563 (81.3%) completed\n",
      "Batch 1280/1563 (81.9%) completed\n",
      "Batch 1290/1563 (82.5%) completed\n",
      "Batch 1300/1563 (83.2%) completed\n",
      "Batch 1310/1563 (83.8%) completed\n",
      "Batch 1320/1563 (84.5%) completed\n",
      "Batch 1330/1563 (85.1%) completed\n",
      "Batch 1340/1563 (85.7%) completed\n",
      "Batch 1350/1563 (86.4%) completed\n",
      "Batch 1360/1563 (87.0%) completed\n",
      "Batch 1370/1563 (87.7%) completed\n",
      "Batch 1380/1563 (88.3%) completed\n",
      "Batch 1390/1563 (88.9%) completed\n",
      "Batch 1400/1563 (89.6%) completed\n",
      "Batch 1410/1563 (90.2%) completed\n",
      "Batch 1420/1563 (90.9%) completed\n",
      "Batch 1430/1563 (91.5%) completed\n",
      "Batch 1440/1563 (92.1%) completed\n",
      "Batch 1450/1563 (92.8%) completed\n",
      "Batch 1460/1563 (93.4%) completed\n",
      "Batch 1470/1563 (94.0%) completed\n",
      "Batch 1480/1563 (94.7%) completed\n",
      "Batch 1490/1563 (95.3%) completed\n",
      "Batch 1500/1563 (96.0%) completed\n",
      "Batch 1510/1563 (96.6%) completed\n",
      "Batch 1520/1563 (97.2%) completed\n",
      "Batch 1530/1563 (97.9%) completed\n",
      "Batch 1540/1563 (98.5%) completed\n",
      "Batch 1550/1563 (99.2%) completed\n",
      "Batch 1560/1563 (99.8%) completed\n",
      "Batch 1563/1563 (100.0%) completed\n",
      "Batch 10/313 (3.2%) completed\n",
      "Batch 20/313 (6.4%) completed\n",
      "Batch 30/313 (9.6%) completed\n",
      "Batch 40/313 (12.8%) completed\n",
      "Batch 50/313 (16.0%) completed\n",
      "Batch 60/313 (19.2%) completed\n",
      "Batch 70/313 (22.4%) completed\n",
      "Batch 80/313 (25.6%) completed\n",
      "Batch 90/313 (28.8%) completed\n",
      "Batch 100/313 (31.9%) completed\n",
      "Batch 110/313 (35.1%) completed\n",
      "Batch 120/313 (38.3%) completed\n",
      "Batch 130/313 (41.5%) completed\n",
      "Batch 140/313 (44.7%) completed\n",
      "Batch 150/313 (47.9%) completed\n",
      "Batch 160/313 (51.1%) completed\n",
      "Batch 170/313 (54.3%) completed\n",
      "Batch 180/313 (57.5%) completed\n",
      "Batch 190/313 (60.7%) completed\n",
      "Batch 200/313 (63.9%) completed\n",
      "Batch 210/313 (67.1%) completed\n",
      "Batch 220/313 (70.3%) completed\n",
      "Batch 230/313 (73.5%) completed\n",
      "Batch 240/313 (76.7%) completed\n",
      "Batch 250/313 (79.9%) completed\n",
      "Batch 260/313 (83.1%) completed\n",
      "Batch 270/313 (86.3%) completed\n",
      "Batch 280/313 (89.5%) completed\n",
      "Batch 290/313 (92.7%) completed\n",
      "Batch 300/313 (95.8%) completed\n",
      "Batch 310/313 (99.0%) completed\n",
      "Batch 313/313 (100.0%) completed\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import cast\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR100\n",
    "\n",
    "# Load DINO ViT-S/16 pre-trained from torch.hub\n",
    "\n",
    "dino_model = cast(\n",
    "    nn.Module,\n",
    "    torch.hub.load(\"facebookresearch/dino:main\", \"dino_vits16\", pretrained=True),\n",
    ")\n",
    "dino_model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dino_model.to(device=device)\n",
    "\n",
    "# Use the preprocess defined in the previous cell\n",
    "# Make sure the dataset uses the correct preprocess\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        # in federated training, we should consider to use mean and std of the cifar100\n",
    "        # these are the parameters on which dino was trained\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = CIFAR100(root=\"./data\", train=True, download=True, transform=preprocess)\n",
    "test_dataset = CIFAR100(root=\"./data\", train=False, download=True, transform=preprocess)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "# Function to extract features from a dataloader\n",
    "def extract_features_and_labels(dataloader, model, device):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        total_batches = len(dataloader)\n",
    "        for batch_idx, (images, labels) in enumerate(dataloader):\n",
    "            images = images.to(device)\n",
    "            # Get features from the backbone (without the classification head)\n",
    "            features = model(images)\n",
    "            all_features.append(features.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "            if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == total_batches:\n",
    "                print(\n",
    "                    f\"Batch {batch_idx + 1}/{total_batches} ({(batch_idx + 1) / total_batches:.1%}) completed\"\n",
    "                )\n",
    "    all_features = torch.cat(all_features, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    return all_features, all_labels\n",
    "\n",
    "\n",
    "# Extract features and labels for train\n",
    "train_features, train_labels = extract_features_and_labels(\n",
    "    train_loader, dino_model, device\n",
    ")\n",
    "torch.save(\n",
    "    {\"features\": train_features, \"labels\": train_labels},\n",
    "    \"features/train_features.pt\",\n",
    ")\n",
    "\n",
    "# Extract features and labels for test\n",
    "test_features, test_labels = extract_features_and_labels(\n",
    "    test_loader, dino_model, device\n",
    ")\n",
    "torch.save(\n",
    "    {\"features\": test_features, \"labels\": test_labels}, \"features/test_features.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2851731a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/einrich99/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomDino(\n",
      "  (backbone): VisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
      "    )\n",
      "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "    (blocks): ModuleList(\n",
      "      (0-11): 12 x Block(\n",
      "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "    (head): Identity()\n",
      "  )\n",
      "  (classifier): Linear(in_features=384, out_features=100, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169M/169M [00:09<00:00, 18.0MB/s] \n",
      "/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/torchvision/datasets/utils.py:216: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  tar.extractall(to_path)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 78\u001b[39m\n\u001b[32m     75\u001b[39m loss.backward()\n\u001b[32m     76\u001b[39m optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m running_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m * images.size(\u001b[32m0\u001b[39m)\n\u001b[32m     79\u001b[39m _, predicted = outputs.max(\u001b[32m1\u001b[39m)\n\u001b[32m     80\u001b[39m total += labels.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from typing import cast\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from fl_task_arithmetic.task import CustomDino\n",
    "from torchvision.datasets import CIFAR100\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Early stopping parameters\n",
    "best_acc = 0\n",
    "patience_counter = 0\n",
    "best_model_state = {}\n",
    "patience = 0\n",
    "\n",
    "\n",
    "dino_pretrained = cast(\n",
    "    nn.Module,\n",
    "    torch.hub.load(\"facebookresearch/dino:main\", \"dino_vits16\", pretrained=True),\n",
    ")\n",
    "model = CustomDino(num_classes=100, backbone=dino_pretrained)\n",
    "print(model)\n",
    "\n",
    "# Example preprocessing for an input image\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        # in federated training, we should consider to use mean and std of the cifar100\n",
    "        # these are the parameters on which dino was trained\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Download and load the CIFAR-100 training and test datasets\n",
    "train_dataset = CIFAR100(root=\"./data\", train=True, download=True, transform=preprocess)\n",
    "test_dataset = CIFAR100(root=\"./data\", train=False, download=True, transform=preprocess)\n",
    "\n",
    "# Freeze backbone\n",
    "for param in model.backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Enable only the head\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True  # Imposta numero di epoche e batch size\n",
    "\n",
    "# DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Optimizer for the head\n",
    "optimizer = torch.optim.Adam(model.classifier.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        print(f\"Epoch {epoch+1}, Iteration {batch_idx+1}\")\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f} - Acc: {epoch_acc:.4f}\"\n",
    "    )\n",
    "\n",
    "    # Evaluation on test set\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += predicted.eq(labels).sum().item()\n",
    "    test_loss /= test_total\n",
    "    test_acc = test_correct / test_total\n",
    "    print(f\"Test Loss: {test_loss:.4f} - Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if epoch == 0:\n",
    "        best_acc = test_acc\n",
    "        patience = 3\n",
    "        patience_counter = 0\n",
    "        best_model_state = model.state_dict()\n",
    "    else:\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                model.load_state_dict(best_model_state)\n",
    "                break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_flower",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
