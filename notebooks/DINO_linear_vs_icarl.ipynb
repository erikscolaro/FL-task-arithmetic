{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e94738e6",
   "metadata": {},
   "source": [
    "# DINO ViT-S/16 Feature Extraction on CIFAR-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "138e2903",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/adrientrahan/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10/1563 (0.6%) completed\n",
      "Batch 20/1563 (1.3%) completed\n",
      "Batch 30/1563 (1.9%) completed\n",
      "Batch 40/1563 (2.6%) completed\n",
      "Batch 50/1563 (3.2%) completed\n",
      "Batch 60/1563 (3.8%) completed\n",
      "Batch 70/1563 (4.5%) completed\n",
      "Batch 80/1563 (5.1%) completed\n",
      "Batch 90/1563 (5.8%) completed\n",
      "Batch 100/1563 (6.4%) completed\n",
      "Batch 110/1563 (7.0%) completed\n",
      "Batch 120/1563 (7.7%) completed\n",
      "Batch 130/1563 (8.3%) completed\n",
      "Batch 140/1563 (9.0%) completed\n",
      "Batch 150/1563 (9.6%) completed\n",
      "Batch 160/1563 (10.2%) completed\n",
      "Batch 170/1563 (10.9%) completed\n",
      "Batch 180/1563 (11.5%) completed\n",
      "Batch 190/1563 (12.2%) completed\n",
      "Batch 200/1563 (12.8%) completed\n",
      "Batch 210/1563 (13.4%) completed\n",
      "Batch 220/1563 (14.1%) completed\n",
      "Batch 230/1563 (14.7%) completed\n",
      "Batch 240/1563 (15.4%) completed\n",
      "Batch 250/1563 (16.0%) completed\n",
      "Batch 260/1563 (16.6%) completed\n",
      "Batch 270/1563 (17.3%) completed\n",
      "Batch 280/1563 (17.9%) completed\n",
      "Batch 290/1563 (18.6%) completed\n",
      "Batch 300/1563 (19.2%) completed\n",
      "Batch 310/1563 (19.8%) completed\n",
      "Batch 320/1563 (20.5%) completed\n",
      "Batch 330/1563 (21.1%) completed\n",
      "Batch 340/1563 (21.8%) completed\n",
      "Batch 350/1563 (22.4%) completed\n",
      "Batch 360/1563 (23.0%) completed\n",
      "Batch 370/1563 (23.7%) completed\n",
      "Batch 380/1563 (24.3%) completed\n",
      "Batch 390/1563 (25.0%) completed\n",
      "Batch 400/1563 (25.6%) completed\n",
      "Batch 410/1563 (26.2%) completed\n",
      "Batch 420/1563 (26.9%) completed\n",
      "Batch 430/1563 (27.5%) completed\n",
      "Batch 440/1563 (28.2%) completed\n",
      "Batch 450/1563 (28.8%) completed\n",
      "Batch 460/1563 (29.4%) completed\n",
      "Batch 470/1563 (30.1%) completed\n",
      "Batch 480/1563 (30.7%) completed\n",
      "Batch 490/1563 (31.3%) completed\n",
      "Batch 500/1563 (32.0%) completed\n",
      "Batch 510/1563 (32.6%) completed\n",
      "Batch 520/1563 (33.3%) completed\n",
      "Batch 530/1563 (33.9%) completed\n",
      "Batch 540/1563 (34.5%) completed\n",
      "Batch 550/1563 (35.2%) completed\n",
      "Batch 560/1563 (35.8%) completed\n",
      "Batch 570/1563 (36.5%) completed\n",
      "Batch 580/1563 (37.1%) completed\n",
      "Batch 590/1563 (37.7%) completed\n",
      "Batch 600/1563 (38.4%) completed\n",
      "Batch 610/1563 (39.0%) completed\n",
      "Batch 620/1563 (39.7%) completed\n",
      "Batch 630/1563 (40.3%) completed\n",
      "Batch 640/1563 (40.9%) completed\n",
      "Batch 650/1563 (41.6%) completed\n",
      "Batch 660/1563 (42.2%) completed\n",
      "Batch 670/1563 (42.9%) completed\n",
      "Batch 680/1563 (43.5%) completed\n",
      "Batch 690/1563 (44.1%) completed\n",
      "Batch 700/1563 (44.8%) completed\n",
      "Batch 710/1563 (45.4%) completed\n",
      "Batch 720/1563 (46.1%) completed\n",
      "Batch 730/1563 (46.7%) completed\n",
      "Batch 740/1563 (47.3%) completed\n",
      "Batch 750/1563 (48.0%) completed\n",
      "Batch 760/1563 (48.6%) completed\n",
      "Batch 770/1563 (49.3%) completed\n",
      "Batch 780/1563 (49.9%) completed\n",
      "Batch 790/1563 (50.5%) completed\n",
      "Batch 800/1563 (51.2%) completed\n",
      "Batch 810/1563 (51.8%) completed\n",
      "Batch 820/1563 (52.5%) completed\n",
      "Batch 830/1563 (53.1%) completed\n",
      "Batch 840/1563 (53.7%) completed\n",
      "Batch 850/1563 (54.4%) completed\n",
      "Batch 860/1563 (55.0%) completed\n",
      "Batch 870/1563 (55.7%) completed\n",
      "Batch 880/1563 (56.3%) completed\n",
      "Batch 890/1563 (56.9%) completed\n",
      "Batch 900/1563 (57.6%) completed\n",
      "Batch 910/1563 (58.2%) completed\n",
      "Batch 920/1563 (58.9%) completed\n",
      "Batch 930/1563 (59.5%) completed\n",
      "Batch 940/1563 (60.1%) completed\n",
      "Batch 950/1563 (60.8%) completed\n",
      "Batch 960/1563 (61.4%) completed\n",
      "Batch 970/1563 (62.1%) completed\n",
      "Batch 980/1563 (62.7%) completed\n",
      "Batch 990/1563 (63.3%) completed\n",
      "Batch 1000/1563 (64.0%) completed\n",
      "Batch 1010/1563 (64.6%) completed\n",
      "Batch 1020/1563 (65.3%) completed\n",
      "Batch 1030/1563 (65.9%) completed\n",
      "Batch 1040/1563 (66.5%) completed\n",
      "Batch 1050/1563 (67.2%) completed\n",
      "Batch 1060/1563 (67.8%) completed\n",
      "Batch 1070/1563 (68.5%) completed\n",
      "Batch 1080/1563 (69.1%) completed\n",
      "Batch 1090/1563 (69.7%) completed\n",
      "Batch 1100/1563 (70.4%) completed\n",
      "Batch 1110/1563 (71.0%) completed\n",
      "Batch 1120/1563 (71.7%) completed\n",
      "Batch 1130/1563 (72.3%) completed\n",
      "Batch 1140/1563 (72.9%) completed\n",
      "Batch 1150/1563 (73.6%) completed\n",
      "Batch 1160/1563 (74.2%) completed\n",
      "Batch 1170/1563 (74.9%) completed\n",
      "Batch 1180/1563 (75.5%) completed\n",
      "Batch 1190/1563 (76.1%) completed\n",
      "Batch 1200/1563 (76.8%) completed\n",
      "Batch 1210/1563 (77.4%) completed\n",
      "Batch 1220/1563 (78.1%) completed\n",
      "Batch 1230/1563 (78.7%) completed\n",
      "Batch 1240/1563 (79.3%) completed\n",
      "Batch 1250/1563 (80.0%) completed\n",
      "Batch 1260/1563 (80.6%) completed\n",
      "Batch 1270/1563 (81.3%) completed\n",
      "Batch 1280/1563 (81.9%) completed\n",
      "Batch 1290/1563 (82.5%) completed\n",
      "Batch 1300/1563 (83.2%) completed\n",
      "Batch 1310/1563 (83.8%) completed\n",
      "Batch 1320/1563 (84.5%) completed\n",
      "Batch 1330/1563 (85.1%) completed\n",
      "Batch 1340/1563 (85.7%) completed\n",
      "Batch 1350/1563 (86.4%) completed\n",
      "Batch 1360/1563 (87.0%) completed\n",
      "Batch 1370/1563 (87.7%) completed\n",
      "Batch 1380/1563 (88.3%) completed\n",
      "Batch 1390/1563 (88.9%) completed\n",
      "Batch 1400/1563 (89.6%) completed\n",
      "Batch 1410/1563 (90.2%) completed\n",
      "Batch 1420/1563 (90.9%) completed\n",
      "Batch 1430/1563 (91.5%) completed\n",
      "Batch 1440/1563 (92.1%) completed\n",
      "Batch 1450/1563 (92.8%) completed\n",
      "Batch 1460/1563 (93.4%) completed\n",
      "Batch 1470/1563 (94.0%) completed\n",
      "Batch 1480/1563 (94.7%) completed\n",
      "Batch 1490/1563 (95.3%) completed\n",
      "Batch 1500/1563 (96.0%) completed\n",
      "Batch 1510/1563 (96.6%) completed\n",
      "Batch 1520/1563 (97.2%) completed\n",
      "Batch 1530/1563 (97.9%) completed\n",
      "Batch 1540/1563 (98.5%) completed\n",
      "Batch 1550/1563 (99.2%) completed\n",
      "Batch 1560/1563 (99.8%) completed\n",
      "Batch 1563/1563 (100.0%) completed\n",
      "Batch 10/313 (3.2%) completed\n",
      "Batch 20/313 (6.4%) completed\n",
      "Batch 30/313 (9.6%) completed\n",
      "Batch 40/313 (12.8%) completed\n",
      "Batch 50/313 (16.0%) completed\n",
      "Batch 60/313 (19.2%) completed\n",
      "Batch 70/313 (22.4%) completed\n",
      "Batch 80/313 (25.6%) completed\n",
      "Batch 90/313 (28.8%) completed\n",
      "Batch 100/313 (31.9%) completed\n",
      "Batch 110/313 (35.1%) completed\n",
      "Batch 120/313 (38.3%) completed\n",
      "Batch 130/313 (41.5%) completed\n",
      "Batch 140/313 (44.7%) completed\n",
      "Batch 150/313 (47.9%) completed\n",
      "Batch 160/313 (51.1%) completed\n",
      "Batch 170/313 (54.3%) completed\n",
      "Batch 180/313 (57.5%) completed\n",
      "Batch 190/313 (60.7%) completed\n",
      "Batch 200/313 (63.9%) completed\n",
      "Batch 210/313 (67.1%) completed\n",
      "Batch 220/313 (70.3%) completed\n",
      "Batch 230/313 (73.5%) completed\n",
      "Batch 240/313 (76.7%) completed\n",
      "Batch 250/313 (79.9%) completed\n",
      "Batch 260/313 (83.1%) completed\n",
      "Batch 270/313 (86.3%) completed\n",
      "Batch 280/313 (89.5%) completed\n",
      "Batch 290/313 (92.7%) completed\n",
      "Batch 300/313 (95.8%) completed\n",
      "Batch 310/313 (99.0%) completed\n",
      "Batch 313/313 (100.0%) completed\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import cast\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR100\n",
    "\n",
    "# Load DINO ViT-S/16 pre-trained from torch.hub\n",
    "\n",
    "dino_model = cast(\n",
    "    nn.Module,\n",
    "    torch.hub.load(\"facebookresearch/dino:main\", \"dino_vits16\", pretrained=True),\n",
    ")\n",
    "dino_model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dino_model.to(device=device)\n",
    "\n",
    "# Use the preprocess defined in the previous cell\n",
    "# Make sure the dataset uses the correct preprocess\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        # in federated training, we should consider to use mean and std of the cifar100\n",
    "        # these are the parameters on which dino was trained\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = CIFAR100(root=\"./data\", train=True, download=True, transform=preprocess)\n",
    "test_dataset = CIFAR100(root=\"./data\", train=False, download=True, transform=preprocess)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "# Function to extract features from a dataloader\n",
    "def extract_features_and_labels(dataloader, model, device):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        total_batches = len(dataloader)\n",
    "        for batch_idx, (images, labels) in enumerate(dataloader):\n",
    "            images = images.to(device)\n",
    "            # Get features from the backbone (without the classification head)\n",
    "            features = model(images)\n",
    "            all_features.append(features.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "            if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == total_batches:\n",
    "                print(\n",
    "                    f\"Batch {batch_idx + 1}/{total_batches} ({(batch_idx + 1) / total_batches:.1%}) completed\"\n",
    "                )\n",
    "    all_features = torch.cat(all_features, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    return all_features, all_labels\n",
    "\n",
    "\n",
    "# Extract features and labels for train\n",
    "train_features, train_labels = extract_features_and_labels(\n",
    "    train_loader, dino_model, device\n",
    ")\n",
    "torch.save(\n",
    "    {\"features\": train_features, \"labels\": train_labels},\n",
    "    \"features/train_features.pt\",\n",
    ")\n",
    "\n",
    "# Extract features and labels for test\n",
    "test_features, test_labels = extract_features_and_labels(\n",
    "    test_loader, dino_model, device\n",
    ")\n",
    "torch.save(\n",
    "    {\"features\": test_features, \"labels\": test_labels}, \"features/test_features.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fd754a",
   "metadata": {},
   "source": [
    "# Linear Classifier Training on DINO Features\n",
    "\n",
    "This section describes the process of training a linear classifier on top of precomputed DINO ViT-S/16 features extracted from the CIFAR-100 dataset. The classifier is trained using early stopping and evaluated on the test split to monitor performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2851731a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/adrientrahan/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 101\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Create a epoch-level progress bar and update it per-batch\u001b[39;00m\n\u001b[1;32m    100\u001b[0m epoch_desc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Training\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch_desc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m progress:\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (features, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;66;03m# Move tensors to device\u001b[39;00m\n\u001b[1;32m    104\u001b[0m         features, labels \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/Documents/ecole/AML/project/FL-task-arithmetic/.venv/lib/python3.10/site-packages/tqdm/notebook.py:234\u001b[0m, in \u001b[0;36mtqdm_notebook.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m unit_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_scale \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    233\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;241m*\u001b[39m unit_scale \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal\n\u001b[0;32m--> 234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_printer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mncols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer\u001b[38;5;241m.\u001b[39mpbar \u001b[38;5;241m=\u001b[39m proxy(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ecole/AML/project/FL-task-arithmetic/.venv/lib/python3.10/site-packages/tqdm/notebook.py:108\u001b[0m, in \u001b[0;36mtqdm_notebook.status_printer\u001b[0;34m(_, total, desc, ncols)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Fallback to text bar if there's no total\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# DEPRECATED: replaced with an 'info' style bar\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# if not total:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Prepare IPython progress bar\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m IProgress \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# #187 #451 #558 #872\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(WARN_NOIPYW)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total:\n\u001b[1;32m    110\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m IProgress(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39mtotal)\n",
      "\u001b[0;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "from typing import Optional, cast\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 10000\n",
    "batch_size = 10000\n",
    "test_batch_size = 1000\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Early stopping parameters\n",
    "best_acc = 0\n",
    "patience_counter = 0\n",
    "best_model_state = {}\n",
    "patience = 10\n",
    "\n",
    "dino_pretrained = cast(\n",
    "    nn.Module,\n",
    "    torch.hub.load(\"facebookresearch/dino:main\", \"dino_vits16\", pretrained=True),\n",
    ")\n",
    "\n",
    "\n",
    "class CustomDino(nn.Module):\n",
    "    def __init__(self, num_classes: int = 100, backbone: Optional[nn.Module] = None):\n",
    "        super().__init__()\n",
    "        if backbone is None:\n",
    "            # Carica DINO senza pretrained e rimuove la head\n",
    "            backbone = cast(\n",
    "                nn.Module,\n",
    "                torch.hub.load(\n",
    "                    \"facebookresearch/dino:main\", \"dino_vits16\", pretrained=False\n",
    "                ),\n",
    "            )\n",
    "        self.backbone: nn.Module = backbone\n",
    "        self.classifier = nn.Linear(\n",
    "            384, num_classes\n",
    "        )  # 384 = output CLS token DINO ViT-S/16\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        features = self.backbone(x)  # [batch, 384]\n",
    "        logits = self.classifier(features)  # [batch, num_classes]\n",
    "        return logits  # , features\n",
    "\n",
    "\n",
    "model = CustomDino(num_classes=100, backbone=dino_pretrained)\n",
    "\n",
    "# Example preprocessing for an input image\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        # in federated training, we should consider to use mean and std of the cifar100\n",
    "        # these are the parameters on which dino was trained\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Load precomputed features and labels\n",
    "train_data = torch.load(\"features/train_features.pt\")\n",
    "test_data = torch.load(\"features/test_features.pt\")\n",
    "\n",
    "train_features, train_labels = train_data[\"features\"], train_data[\"labels\"]\n",
    "test_features, test_labels = test_data[\"features\"], test_data[\"labels\"]\n",
    "\n",
    "# Create TensorDatasets and DataLoaders from features\n",
    "train_dataset = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=test_batch_size)\n",
    "\n",
    "# Optimizer for the head\n",
    "optimizer = torch.optim.Adam(model.classifier.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -----------------------------------\n",
    "# Train only the linear head for fun startin from the features\n",
    "# -----------------------------------\n",
    "\n",
    "complete_model = model\n",
    "model = model.classifier\n",
    "model.to(device=device)\n",
    "best_model = model\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Create a epoch-level progress bar and update it per-batch\n",
    "    epoch_desc = f\"Epoch {epoch+1} Training\"\n",
    "    with tqdm(total=len(train_loader), desc=epoch_desc, leave=True) as progress:\n",
    "        for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "            # Move tensors to device\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * features.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            # Update the progress bar with running metrics\n",
    "            batch_loss = running_loss / total if total > 0 else 0\n",
    "            batch_acc = correct / total if total > 0 else 0\n",
    "            progress.set_postfix(\n",
    "                {\"loss\": f\"{batch_loss:.4f}\", \"acc\": f\"{batch_acc:.4f}\"}\n",
    "            )\n",
    "            progress.update(1)\n",
    "\n",
    "    epoch_loss = running_loss / total if total > 0 else 0\n",
    "    epoch_acc = correct / total if total > 0 else 0\n",
    "\n",
    "    # Evaluation on test set\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_acc = 0.0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "\n",
    "    # Create a per-epoch evaluation progress bar\n",
    "    eval_desc = f\"Epoch {epoch+1} Evaluation\"\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(test_loader), desc=eval_desc, leave=True) as test_progress:\n",
    "            for features, labels in test_loader:\n",
    "                features, labels = features.to(device), labels.to(device)\n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item() * features.size(0)\n",
    "                _, predicted = outputs.max(1)\n",
    "                test_total += labels.size(0)\n",
    "                test_correct += predicted.eq(labels).sum().item()\n",
    "                test_progress.update(1)\n",
    "\n",
    "            test_loss = test_loss / test_total if test_total > 0 else 0\n",
    "            test_acc = test_correct / test_total if test_total > 0 else 0\n",
    "\n",
    "            test_progress.set_postfix(\n",
    "                {\n",
    "                    \"loss\": f\"{test_loss / test_total if test_total > 0 else 0:.4f}\",\n",
    "                    \"acc\": f\"{test_acc:.4f}\",\n",
    "                }\n",
    "            )\n",
    "            test_progress.update(1)\n",
    "\n",
    "    # Early stopping\n",
    "    if epoch == 0:\n",
    "        best_acc = test_acc\n",
    "        if patience == 0:\n",
    "            print(\"Default patience = 3\")\n",
    "            patience = 3\n",
    "        patience_counter = 0\n",
    "        best_model_state = model.state_dict()\n",
    "    else:\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                model.load_state_dict(best_model_state)\n",
    "                break\n",
    "\n",
    "torch.save(best_model_state, \"./linear_classifier.pth\")\n",
    "\n",
    "print(\n",
    "    f\"Best model statistics:\\nAccuracy: {best_acc:.4f}\\nPatience reached: {patience_counter}\\nModel state dict keys: {list(best_model_state.keys())}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f49b79",
   "metadata": {},
   "source": [
    "# Class Centroid Computation and Exemplar Selection\n",
    "\n",
    "This cell computes normalized class centroids from DINO features by randomly selecting a fixed number of exemplars per class. The centroids are saved for use in downstream tasks such as nearest centroid classification or incremental learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e627208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Configuration\n",
    "num_exemplars = 100\n",
    "\n",
    "# Load precomputed features\n",
    "train_data = torch.load(\"features/train_features.pt\")\n",
    "train_features = train_data[\"features\"].cpu()\n",
    "train_labels = train_data[\"labels\"].cpu()\n",
    "\n",
    "# Compute normalized mean vector (centroid) for each class\n",
    "unique_classes = torch.unique(train_labels).tolist()\n",
    "centroids = {}\n",
    "rng = torch.Generator().manual_seed(42)\n",
    "\n",
    "for cls in unique_classes:\n",
    "    # Extract all features for this class\n",
    "    class_mask = train_labels == cls\n",
    "    class_features = train_features[class_mask]\n",
    "\n",
    "    # Select num_exemplars randomly\n",
    "    n = class_features.size(0)\n",
    "    k = min(num_exemplars, n)\n",
    "    indices = torch.randperm(n, generator=rng)[:k]\n",
    "    exemplars = class_features[indices]\n",
    "\n",
    "    # Compute normalized mean (centroid)\n",
    "    centroid = F.normalize(exemplars.mean(dim=0, keepdim=True), p=2, dim=1).squeeze(0)\n",
    "    centroids[cls] = centroid\n",
    "\n",
    "# Save to disk\n",
    "torch.save(\n",
    "    {\"class\": unique_classes, \"centroid\": centroids},\n",
    "    \"./class_centroids.pth\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d6019d",
   "metadata": {},
   "source": [
    "## Nearest Centroid Classification Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93063e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def nearest_neighbor(class_centroids, feature_vector):\n",
    "    \"\"\"\n",
    "    Find the nearest class by computing distances to all centroids.\n",
    "\n",
    "    Args:\n",
    "        class_centroids: dict {class_label: centroid_tensor[384]}\n",
    "        feature_vector: torch.Tensor of shape [384]\n",
    "\n",
    "    Returns:\n",
    "        predicted_class: int\n",
    "    \"\"\"\n",
    "    min_distance = float(\"inf\")\n",
    "    predicted_class = None\n",
    "\n",
    "    for cls, centroid in class_centroids.items():\n",
    "        # Compute Euclidean distance\n",
    "        distance = torch.linalg.vector_norm(feature_vector - centroid).item()\n",
    "\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            predicted_class = cls\n",
    "\n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07c34dca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c37155dd1a34448a791b0d458dcc7e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Nearest Centroid Evaluation:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Neighbor Accuracy: 60.46% (6046/10000)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Load test features\n",
    "test_data = torch.load(\"features/test_features.pt\")\n",
    "test_features = test_data[\"features\"].cpu()\n",
    "test_labels = test_data[\"labels\"].cpu()\n",
    "\n",
    "# Load class centroids\n",
    "centroids_data = torch.load(\"./class_centroids.pth\")\n",
    "centroids = centroids_data[\"centroid\"]\n",
    "centroid_labels = centroids_data[\"class\"]\n",
    "\n",
    "# Evaluate accuracy using nearest neighbor\n",
    "correct = 0\n",
    "total = test_features.size(0)\n",
    "\n",
    "for i in tqdm(range(total), desc=\"Nearest Centroid Evaluation\"):\n",
    "    pred_idx = nearest_neighbor(centroids, test_features[i])\n",
    "    pred = centroid_labels[pred_idx]\n",
    "    if pred == test_labels[i].item():\n",
    "        correct += 1\n",
    "\n",
    "accuracy = 100.0 * correct / total\n",
    "print(f\"Nearest Neighbor Accuracy: {accuracy:.2f}% ({correct}/{total})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd231e27",
   "metadata": {},
   "source": [
    "# Convert the centroids in linear layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6721ece7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl-task-arithmetic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
