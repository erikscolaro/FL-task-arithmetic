{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afbedf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading project configuration... \n",
      "Success\n",
      "Wandb config:\n",
      "\tentity=aml-fl-project\n",
      "\tproject=fl-task-arithmetic\n",
      "\tgroup=experiment-test\n",
      "\tnotes=Test\n",
      "\tresume=allow\n",
      "\trun_id=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/wandb/analytics/sentry.py:279: DeprecationWarning: The `Scope.user` setter is deprecated in favor of `Scope.set_user()`.\n",
      "  self.scope.user = {\"email\": email}\n",
      "wandb: Currently logged in as: erikscolaro31 (aml-fl-project) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
      "/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/wandb/analytics/sentry.py:279: DeprecationWarning: The `Scope.user` setter is deprecated in favor of `Scope.set_user()`.\n",
      "  self.scope.user = {\"email\": email}\n",
      "wandb: setting up run fl-task-arithmetic-experiment-test-0-server\n",
      "/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/wandb/analytics/sentry.py:279: DeprecationWarning: The `Scope.user` setter is deprecated in favor of `Scope.set_user()`.\n",
      "  self.scope.user = {\"email\": email}\n",
      "wandb: Tracking run with wandb version 0.23.0\n",
      "wandb: Run data is saved locally in /home/einrich99/Progetti/FL-task-arithmetic/wandb/run-20251128_180011-fl-task-arithmetic-experiment-test-0-server\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Resuming run server\n",
      "wandb: â­ï¸ View project at https://wandb.ai/aml-fl-project/fl-task-arithmetic\n",
      "wandb: ðŸš€ View run at https://wandb.ai/aml-fl-project/fl-task-arithmetic/runs/fl-task-arithmetic-experiment-test-0-server\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint not found on WandB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/google/protobuf/internal/well_known_types.py:174: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  self.FromDatetime(datetime.datetime.utcnow())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I find a previous run on the cloud. I'll resume from round 2.\n",
      "correcly created the state dict for the global model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      Starting CustomFedAvg strategy:\n",
      "\u001b[92mINFO \u001b[0m:      \tâ”œâ”€â”€ Number of rounds: 20\n",
      "\u001b[92mINFO \u001b[0m:      \tâ”œâ”€â”€ ArrayRecord (0.27 MB)\n",
      "\u001b[92mINFO \u001b[0m:      \tâ”œâ”€â”€ ConfigRecord (train): {'lr': 0.01, 'server-round': 2}\n",
      "\u001b[92mINFO \u001b[0m:      \tâ”œâ”€â”€ ConfigRecord (evaluate): (empty!)\n",
      "\u001b[92mINFO \u001b[0m:      \tâ”œâ”€â”€> Sampling:\n",
      "\u001b[92mINFO \u001b[0m:      \tâ”‚\tâ”œâ”€â”€Fraction: train (0.50) | evaluate ( 1.00)\n",
      "\u001b[92mINFO \u001b[0m:      \tâ”‚\tâ”œâ”€â”€Minimum nodes: train (2) | evaluate (2)\n",
      "\u001b[92mINFO \u001b[0m:      \tâ”‚\tâ””â”€â”€Minimum available nodes: 2\n",
      "\u001b[92mINFO \u001b[0m:      \tâ””â”€â”€> Keys in records:\n",
      "\u001b[92mINFO \u001b[0m:      \t\tâ”œâ”€â”€ Weighted by: 'num-examples'\n",
      "\u001b[92mINFO \u001b[0m:      \t\tâ”œâ”€â”€ ArrayRecord key: 'arrays'\n",
      "\u001b[92mINFO \u001b[0m:      \t\tâ””â”€â”€ ConfigRecord key: 'config'\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 1/20]\n",
      "\u001b[92mINFO \u001b[0m:      configure_train: Sampled 5 nodes (out of 10)\n",
      "\u001b[36m(ClientAppActor pid=10697)\u001b[0m /home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr_datasets/partitioner/pathological_partitioner.py:188: UserWarning: Classes: [0, 1, 2, 3, 5, 6, 7, 10, 11, 13, 14, 17, 19, 20, 21, 23, 24, 25, 26, 28, 29, 30, 32, 34, 35, 38, 40, 42, 45, 46, 48, 50, 54, 55, 56, 57, 58, 59, 60, 61, 63, 65, 66, 68, 69, 71, 73, 79, 83, 84, 85, 86, 88, 90, 91, 94, 96, 97, 98] will NOT be used due to the chosen configuration. If it is undesired behavior consider setting 'first_class_deterministic_assignment=True' which in case when the number of classes is smaller than the number of partitions will utilize all the classes for the created partitions.\n",
      "\u001b[36m(ClientAppActor pid=10697)\u001b[0m   warnings.warn(\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_train: Received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \tâ””â”€â”€> Aggregated MetricRecord: {'train_loss': 6.588237297488228}\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: Sampled 10 nodes (out of 10)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: Received 10 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \tâ””â”€â”€> Aggregated MetricRecord: {'eval_loss': 4.4539861460101235, 'eval_acc': 0.02892673695593404}\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 2/20]\n",
      "\u001b[92mINFO \u001b[0m:      configure_train: Sampled 5 nodes (out of 10)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_train: Received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \tâ””â”€â”€> Aggregated MetricRecord: {'train_loss': 7.783125095673697}\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: Sampled 10 nodes (out of 10)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: Received 10 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \tâ””â”€â”€> Aggregated MetricRecord: {'eval_loss': 4.239862013394189, 'eval_acc': 0.031089483644228172}\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 3/20]\n",
      "\u001b[92mINFO \u001b[0m:      configure_train: Sampled 5 nodes (out of 10)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_train: Received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \tâ””â”€â”€> Aggregated MetricRecord: {'train_loss': 7.62168596272752}\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: Sampled 10 nodes (out of 10)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: Received 10 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \tâ””â”€â”€> Aggregated MetricRecord: {'eval_loss': 4.344646018010134, 'eval_acc': 0.027304676939713435}\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 4/20]\n",
      "\u001b[92mINFO \u001b[0m:      configure_train: Sampled 5 nodes (out of 10)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_train: Received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \tâ””â”€â”€> Aggregated MetricRecord: {'train_loss': 6.965866578168369}\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: Sampled 10 nodes (out of 10)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: Received 10 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \tâ””â”€â”€> Aggregated MetricRecord: {'eval_loss': 4.655157155165073, 'eval_acc': 0.0283860502838605}\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 5/20]\n",
      "\u001b[92mINFO \u001b[0m:      configure_train: Sampled 5 nodes (out of 10)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_train: Received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \tâ””â”€â”€> Aggregated MetricRecord: {'train_loss': 6.729918701784889}\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: Sampled 10 nodes (out of 10)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: Received 10 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \tâ””â”€â”€> Aggregated MetricRecord: {'eval_loss': 4.650220184433123, 'eval_acc': 0.026223303595566367}\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 6/20]\n",
      "\u001b[92mINFO \u001b[0m:      configure_train: Sampled 5 nodes (out of 10)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_train: Received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \tâ””â”€â”€> Aggregated MetricRecord: {'train_loss': 7.256722134271756}\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: Sampled 10 nodes (out of 10)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: Received 10 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \tâ””â”€â”€> Aggregated MetricRecord: {'eval_loss': 4.547336649463276, 'eval_acc': 0.027845363611786966}\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 7/20]\n",
      "\u001b[92mINFO \u001b[0m:      configure_train: Sampled 5 nodes (out of 10)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_train: Received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \tâ””â”€â”€> Aggregated MetricRecord: {'train_loss': 6.620318320116793}\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: Sampled 10 nodes (out of 10)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: Received 10 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \tâ””â”€â”€> Aggregated MetricRecord: {'eval_loss': 4.554961080484441, 'eval_acc': 0.039740470397404706}\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 8/20]\n",
      "\u001b[92mINFO \u001b[0m:      configure_train: Sampled 5 nodes (out of 10)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "run_config = \"conf1.toml\"\n",
    "\n",
    "command = f\"cd .. && flwr run --run-config config/{run_config}\"\n",
    "os.system(command)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_flower",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
