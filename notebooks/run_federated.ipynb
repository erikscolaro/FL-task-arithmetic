{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afbedf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading project configuration... \n",
      "Success\n",
      "Wandb config:\n",
      "\tentity=aml-fl-project\n",
      "\tproject=fl-task-arithmetic\n",
      "\tgroup=experiment-test3\n",
      "\tnotes=Test\n",
      "\tresume=allow\n",
      "\trun_id=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: erikscolaro31 (aml-fl-project) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
      "/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/wandb/analytics/sentry.py:279: DeprecationWarning: The `Scope.user` setter is deprecated in favor of `Scope.set_user()`.\n",
      "  self.scope.user = {\"email\": email}\n",
      "wandb: setting up run fl-task-arithmetic-experiment-test3-0-server\n",
      "/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/wandb/analytics/sentry.py:279: DeprecationWarning: The `Scope.user` setter is deprecated in favor of `Scope.set_user()`.\n",
      "  self.scope.user = {\"email\": email}\n",
      "wandb: Tracking run with wandb version 0.23.0\n",
      "wandb: Run data is saved locally in /home/einrich99/Progetti/FL-task-arithmetic/wandb/run-20251216_164938-fl-task-arithmetic-experiment-test3-0-server\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Resuming run server\n",
      "wandb: â­ï¸ View project at https://wandb.ai/aml-fl-project/fl-task-arithmetic\n",
      "wandb: ðŸš€ View run at https://wandb.ai/aml-fl-project/fl-task-arithmetic/runs/fl-task-arithmetic-experiment-test3-0-server\n",
      "Using cache found in /home/einrich99/.cache/torch/hub/facebookresearch_dino_main\n",
      "/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/google/protobuf/internal/well_known_types.py:174: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  self.FromDatetime(datetime.datetime.utcnow())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading iCaRL classifier from local cache: /home/einrich99/Progetti/FL-task-arithmetic/utilities/trained/nearest_centroid_classifier.pth\n",
      "I find a previous run on the cloud. I'll resume from round 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Downloading large artifact 'experiment-test3-models:latest', 82.84MB. 1 files...\n",
      "wandb:   1 of 1 files downloaded.  \n",
      "Done. 00:00:01.3 (62.4MB/s)\n",
      "\u001b[92mINFO \u001b[0m:      Starting CustomFedAvg strategy:\n",
      "\u001b[92mINFO \u001b[0m:      \tâ”œâ”€â”€ Number of rounds: 5\n",
      "\u001b[92mINFO \u001b[0m:      \tâ”œâ”€â”€ ArrayRecord (82.81 MB)\n",
      "\u001b[92mINFO \u001b[0m:      \tâ”œâ”€â”€ ConfigRecord (train): {'lr': 0.001}\n",
      "\u001b[92mINFO \u001b[0m:      \tâ”œâ”€â”€ ConfigRecord (evaluate): (empty!)\n",
      "\u001b[92mINFO \u001b[0m:      \tâ”œâ”€â”€> Sampling:\n",
      "\u001b[92mINFO \u001b[0m:      \tâ”‚\tâ”œâ”€â”€Fraction: train (0.20) | evaluate ( 1.00)\n",
      "\u001b[92mINFO \u001b[0m:      \tâ”‚\tâ”œâ”€â”€Minimum nodes: train (2) | evaluate (2)\n",
      "\u001b[92mINFO \u001b[0m:      \tâ”‚\tâ””â”€â”€Minimum available nodes: 2\n",
      "\u001b[92mINFO \u001b[0m:      \tâ””â”€â”€> Keys in records:\n",
      "\u001b[92mINFO \u001b[0m:      \t\tâ”œâ”€â”€ Weighted by: 'num-examples'\n",
      "\u001b[92mINFO \u001b[0m:      \t\tâ”œâ”€â”€ ArrayRecord key: 'arrays'\n",
      "\u001b[92mINFO \u001b[0m:      \t\tâ””â”€â”€ ConfigRecord key: 'config'\n",
      "\u001b[92mINFO \u001b[0m:      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/einrich99/Progetti/FL-task-arithmetic/artifacts/experiment-test3-models:v2/model.pth\n",
      "Successfully loaded model from: /home/einrich99/Progetti/FL-task-arithmetic/artifacts/experiment-test3-models:v2/model.pth\n",
      "correcly created the state dict for the global model\n",
      "Server round: 0\n",
      "Error obtaining the test split from the federated dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      Initial global evaluation results: {}\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 1/5]\n",
      "\u001b[92mINFO \u001b[0m:      configure_train: Sampled 2 nodes (out of 2)\n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m Using cache found in /home/einrich99/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m Loading iCaRL classifier from local cache: /home/einrich99/Progetti/FL-task-arithmetic/utilities/trained/nearest_centroid_classifier.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m /home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr_datasets/partitioner/pathological_partitioner.py:188: UserWarning: Classes: [0, 1, 2, 3, 6, 9, 10, 11, 14, 16, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 31, 32, 33, 35, 37, 38, 40, 41, 42, 43, 46, 48, 50, 52, 55, 56, 57, 59, 60, 64, 65, 66, 67, 72, 76, 77, 78, 80, 81, 82, 85, 87, 88, 90, 91, 92, 93, 95, 96, 97, 98, 99] will NOT be used due to the chosen configuration. If it is undesired behavior consider setting 'first_class_deterministic_assignment=True' which in case when the number of classes is smaller than the number of partitions will utilize all the classes for the created partitions.\n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m [Client 0] Using sparse fine-tuning\n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m === Step 1: Calibrating Gradient Masks ===\n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m Starting mask calibration with 5 rounds...\n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m Sparsity ratio: 90.00% (freezing 90.00% of parameters)\n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m Calibration round 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=33947, ip=192.168.1.105, actor_id=7a8922abd9faa0047673b1d401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7b4ac0956c60>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/clientapp/client_app.py\", line 161, in __call__\n",
      "    return self._registered_funcs[full_name](message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/client_app.py\", line 39, in train\n",
      "    train_loss = train_sparse_fn(\n",
      "                 ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/task.py\", line 171, in train_sparse\n",
      "    masks = calibrate_gradient_masks(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/sensitivity.py\", line 94, in calibrate_gradient_masks\n",
      "    threshold = torch.quantile(all_scores, sparsity_ratio)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: quantile() input tensor is too large\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=33947, ip=192.168.1.105, actor_id=7a8922abd9faa0047673b1d401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7b4ac0956c60>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.clientapp.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: quantile() input tensor is too large\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 124, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 189, in process_message\n",
      "    raise ex\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 176, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=33947, ip=192.168.1.105, actor_id=7a8922abd9faa0047673b1d401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7b4ac0956c60>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/clientapp/client_app.py\", line 161, in __call__\n",
      "    return self._registered_funcs[full_name](message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/client_app.py\", line 39, in train\n",
      "    train_loss = train_sparse_fn(\n",
      "                 ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/task.py\", line 171, in train_sparse\n",
      "    masks = calibrate_gradient_masks(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/sensitivity.py\", line 94, in calibrate_gradient_masks\n",
      "    threshold = torch.quantile(all_scores, sparsity_ratio)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: quantile() input tensor is too large\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=33947, ip=192.168.1.105, actor_id=7a8922abd9faa0047673b1d401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7b4ac0956c60>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.clientapp.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: quantile() input tensor is too large\n",
      "\n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m Using cache found in /home/einrich99/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m Loading iCaRL classifier from local cache: /home/einrich99/Progetti/FL-task-arithmetic/utilities/trained/nearest_centroid_classifier.pth\n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m [Client 1] Using sparse fine-tuning\n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m === Step 1: Calibrating Gradient Masks ===\n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m Starting mask calibration with 5 rounds...\n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m Sparsity ratio: 90.00% (freezing 90.00% of parameters)\n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m Calibration round 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=33947, ip=192.168.1.105, actor_id=7a8922abd9faa0047673b1d401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7b4ac0956c60>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/clientapp/client_app.py\", line 161, in __call__\n",
      "    return self._registered_funcs[full_name](message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/client_app.py\", line 39, in train\n",
      "    train_loss = train_sparse_fn(\n",
      "                 ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/task.py\", line 171, in train_sparse\n",
      "    masks = calibrate_gradient_masks(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/sensitivity.py\", line 94, in calibrate_gradient_masks\n",
      "    threshold = torch.quantile(all_scores, sparsity_ratio)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: quantile() input tensor is too large\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=33947, ip=192.168.1.105, actor_id=7a8922abd9faa0047673b1d401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7b4ac0956c60>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.clientapp.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: quantile() input tensor is too large\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 124, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 189, in process_message\n",
      "    raise ex\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 176, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=33947, ip=192.168.1.105, actor_id=7a8922abd9faa0047673b1d401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7b4ac0956c60>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/clientapp/client_app.py\", line 161, in __call__\n",
      "    return self._registered_funcs[full_name](message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/client_app.py\", line 39, in train\n",
      "    train_loss = train_sparse_fn(\n",
      "                 ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/task.py\", line 171, in train_sparse\n",
      "    masks = calibrate_gradient_masks(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/sensitivity.py\", line 94, in calibrate_gradient_masks\n",
      "    threshold = torch.quantile(all_scores, sparsity_ratio)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: quantile() input tensor is too large\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=33947, ip=192.168.1.105, actor_id=7a8922abd9faa0047673b1d401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7b4ac0956c60>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.clientapp.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: quantile() input tensor is too large\n",
      "\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_train: Received 0 results and 2 failures\n",
      "\u001b[92mINFO \u001b[0m:      \t> Received error in reply from node 9088339682545869223: <class 'ray.exceptions.RayTaskError(ClientAppException)'>:<'\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=33947, ip=192.168.1.105, actor_id=7a8922abd9faa0047673b1d401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7b4ac0956c60>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/clientapp/client_app.py\", line 161, in __call__\n",
      "    return self._registered_funcs[full_name](message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/client_app.py\", line 39, in train\n",
      "    train_loss = train_sparse_fn(\n",
      "                 ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/task.py\", line 171, in train_sparse\n",
      "    masks = calibrate_gradient_masks(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/sensitivity.py\", line 94, in calibrate_gradient_masks\n",
      "    threshold = torch.quantile(all_scores, sparsity_ratio)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: quantile() input tensor is too large\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=33947, ip=192.168.1.105, actor_id=7a8922abd9faa0047673b1d401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7b4ac0956c60>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.clientapp.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: quantile() input tensor is too large'>\n",
      "\u001b[92mINFO \u001b[0m:      \t> Received error in reply from node 16040892530603135789: <class 'ray.exceptions.RayTaskError(ClientAppException)'>:<'\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=33947, ip=192.168.1.105, actor_id=7a8922abd9faa0047673b1d401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7b4ac0956c60>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/clientapp/client_app.py\", line 161, in __call__\n",
      "    return self._registered_funcs[full_name](message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/client_app.py\", line 39, in train\n",
      "    train_loss = train_sparse_fn(\n",
      "                 ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/task.py\", line 171, in train_sparse\n",
      "    masks = calibrate_gradient_masks(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/sensitivity.py\", line 94, in calibrate_gradient_masks\n",
      "    threshold = torch.quantile(all_scores, sparsity_ratio)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: quantile() input tensor is too large\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=33947, ip=192.168.1.105, actor_id=7a8922abd9faa0047673b1d401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7b4ac0956c60>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.clientapp.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: quantile() input tensor is too large'>\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: Sampled 2 nodes (out of 2)\n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m Using cache found in /home/einrich99/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m Loading iCaRL classifier from local cache: /home/einrich99/Progetti/FL-task-arithmetic/utilities/trained/nearest_centroid_classifier.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m Using cache found in /home/einrich99/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m Loading iCaRL classifier from local cache: /home/einrich99/Progetti/FL-task-arithmetic/utilities/trained/nearest_centroid_classifier.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: Received 2 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \tâ””â”€â”€> Aggregated MetricRecord: {'eval_loss': 1.618479693905702, 'eval_acc': 0.6295673076923077}\n",
      "\u001b[92mINFO \u001b[0m:      Global evaluation\n",
      "\u001b[92mINFO \u001b[0m:      \tâ””â”€â”€> MetricRecord: {}\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 2/5]\n",
      "\u001b[92mINFO \u001b[0m:      configure_train: Sampled 2 nodes (out of 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server round: 1\n",
      "Error obtaining the test split from the federated dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m Using cache found in /home/einrich99/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m Loading iCaRL classifier from local cache: /home/einrich99/Progetti/FL-task-arithmetic/utilities/trained/nearest_centroid_classifier.pth\n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m [Client 0] Using sparse fine-tuning\n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m === Step 1: Calibrating Gradient Masks ===\n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m Starting mask calibration with 5 rounds...\n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m Sparsity ratio: 90.00% (freezing 90.00% of parameters)\n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m Calibration round 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=33947, ip=192.168.1.105, actor_id=7a8922abd9faa0047673b1d401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7b4ac0956c60>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/clientapp/client_app.py\", line 161, in __call__\n",
      "    return self._registered_funcs[full_name](message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/client_app.py\", line 39, in train\n",
      "    train_loss = train_sparse_fn(\n",
      "                 ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/task.py\", line 171, in train_sparse\n",
      "    masks = calibrate_gradient_masks(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/sensitivity.py\", line 94, in calibrate_gradient_masks\n",
      "    threshold = torch.quantile(all_scores, sparsity_ratio)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: quantile() input tensor is too large\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=33947, ip=192.168.1.105, actor_id=7a8922abd9faa0047673b1d401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7b4ac0956c60>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.clientapp.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: quantile() input tensor is too large\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 124, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 189, in process_message\n",
      "    raise ex\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 176, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=33947, ip=192.168.1.105, actor_id=7a8922abd9faa0047673b1d401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7b4ac0956c60>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/clientapp/client_app.py\", line 161, in __call__\n",
      "    return self._registered_funcs[full_name](message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/client_app.py\", line 39, in train\n",
      "    train_loss = train_sparse_fn(\n",
      "                 ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/task.py\", line 171, in train_sparse\n",
      "    masks = calibrate_gradient_masks(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/sensitivity.py\", line 94, in calibrate_gradient_masks\n",
      "    threshold = torch.quantile(all_scores, sparsity_ratio)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: quantile() input tensor is too large\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=33947, ip=192.168.1.105, actor_id=7a8922abd9faa0047673b1d401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7b4ac0956c60>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.clientapp.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: quantile() input tensor is too large\n",
      "\n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m Using cache found in /home/einrich99/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m Loading iCaRL classifier from local cache: /home/einrich99/Progetti/FL-task-arithmetic/utilities/trained/nearest_centroid_classifier.pth\n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m [Client 1] Using sparse fine-tuning\n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m === Step 1: Calibrating Gradient Masks ===\n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m Starting mask calibration with 5 rounds...\n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m Sparsity ratio: 90.00% (freezing 90.00% of parameters)\n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m Calibration round 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=33947, ip=192.168.1.105, actor_id=7a8922abd9faa0047673b1d401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7b4ac0956c60>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/clientapp/client_app.py\", line 161, in __call__\n",
      "    return self._registered_funcs[full_name](message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/client_app.py\", line 39, in train\n",
      "    train_loss = train_sparse_fn(\n",
      "                 ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/task.py\", line 171, in train_sparse\n",
      "    masks = calibrate_gradient_masks(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/sensitivity.py\", line 94, in calibrate_gradient_masks\n",
      "    threshold = torch.quantile(all_scores, sparsity_ratio)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: quantile() input tensor is too large\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=33947, ip=192.168.1.105, actor_id=7a8922abd9faa0047673b1d401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7b4ac0956c60>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.clientapp.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: quantile() input tensor is too large\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 124, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 189, in process_message\n",
      "    raise ex\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 176, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=33947, ip=192.168.1.105, actor_id=7a8922abd9faa0047673b1d401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7b4ac0956c60>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/clientapp/client_app.py\", line 161, in __call__\n",
      "    return self._registered_funcs[full_name](message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/client_app.py\", line 39, in train\n",
      "    train_loss = train_sparse_fn(\n",
      "                 ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/task.py\", line 171, in train_sparse\n",
      "    masks = calibrate_gradient_masks(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/sensitivity.py\", line 94, in calibrate_gradient_masks\n",
      "    threshold = torch.quantile(all_scores, sparsity_ratio)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: quantile() input tensor is too large\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=33947, ip=192.168.1.105, actor_id=7a8922abd9faa0047673b1d401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7b4ac0956c60>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.clientapp.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: quantile() input tensor is too large\n",
      "\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_train: Received 0 results and 2 failures\n",
      "\u001b[92mINFO \u001b[0m:      \t> Received error in reply from node 9088339682545869223: <class 'ray.exceptions.RayTaskError(ClientAppException)'>:<'\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=33947, ip=192.168.1.105, actor_id=7a8922abd9faa0047673b1d401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7b4ac0956c60>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/clientapp/client_app.py\", line 161, in __call__\n",
      "    return self._registered_funcs[full_name](message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/client_app.py\", line 39, in train\n",
      "    train_loss = train_sparse_fn(\n",
      "                 ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/task.py\", line 171, in train_sparse\n",
      "    masks = calibrate_gradient_masks(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/sensitivity.py\", line 94, in calibrate_gradient_masks\n",
      "    threshold = torch.quantile(all_scores, sparsity_ratio)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: quantile() input tensor is too large\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=33947, ip=192.168.1.105, actor_id=7a8922abd9faa0047673b1d401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7b4ac0956c60>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.clientapp.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: quantile() input tensor is too large'>\n",
      "\u001b[92mINFO \u001b[0m:      \t> Received error in reply from node 16040892530603135789: <class 'ray.exceptions.RayTaskError(ClientAppException)'>:<'\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=33947, ip=192.168.1.105, actor_id=7a8922abd9faa0047673b1d401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7b4ac0956c60>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/clientapp/client_app.py\", line 161, in __call__\n",
      "    return self._registered_funcs[full_name](message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/client_app.py\", line 39, in train\n",
      "    train_loss = train_sparse_fn(\n",
      "                 ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/task.py\", line 171, in train_sparse\n",
      "    masks = calibrate_gradient_masks(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/sensitivity.py\", line 94, in calibrate_gradient_masks\n",
      "    threshold = torch.quantile(all_scores, sparsity_ratio)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: quantile() input tensor is too large\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=33947, ip=192.168.1.105, actor_id=7a8922abd9faa0047673b1d401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7b4ac0956c60>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.clientapp.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: quantile() input tensor is too large'>\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: Sampled 2 nodes (out of 2)\n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m Using cache found in /home/einrich99/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m Loading iCaRL classifier from local cache: /home/einrich99/Progetti/FL-task-arithmetic/utilities/trained/nearest_centroid_classifier.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m Using cache found in /home/einrich99/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m Loading iCaRL classifier from local cache: /home/einrich99/Progetti/FL-task-arithmetic/utilities/trained/nearest_centroid_classifier.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: Received 2 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \tâ””â”€â”€> Aggregated MetricRecord: {'eval_loss': 1.618479693905702, 'eval_acc': 0.6295673076923077}\n",
      "\u001b[92mINFO \u001b[0m:      Global evaluation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server round: 2\n",
      "Model saved to WandB as artifact 'experiment-test3-models'.\n",
      "Error obtaining the test split from the federated dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      \tâ””â”€â”€> MetricRecord: {}\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 3/5]\n",
      "\u001b[92mINFO \u001b[0m:      configure_train: Sampled 2 nodes (out of 2)\n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m Using cache found in /home/einrich99/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m Loading iCaRL classifier from local cache: /home/einrich99/Progetti/FL-task-arithmetic/utilities/trained/nearest_centroid_classifier.pth\n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m [Client 0] Using sparse fine-tuning\n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m === Step 1: Calibrating Gradient Masks ===\n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m Starting mask calibration with 5 rounds...\n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m Sparsity ratio: 90.00% (freezing 90.00% of parameters)\n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m Calibration round 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=33947, ip=192.168.1.105, actor_id=7a8922abd9faa0047673b1d401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7b4ac0956c60>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/clientapp/client_app.py\", line 161, in __call__\n",
      "    return self._registered_funcs[full_name](message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/client_app.py\", line 39, in train\n",
      "    train_loss = train_sparse_fn(\n",
      "                 ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/task.py\", line 171, in train_sparse\n",
      "    masks = calibrate_gradient_masks(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/sensitivity.py\", line 94, in calibrate_gradient_masks\n",
      "    threshold = torch.quantile(all_scores, sparsity_ratio)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: quantile() input tensor is too large\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=33947, ip=192.168.1.105, actor_id=7a8922abd9faa0047673b1d401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7b4ac0956c60>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.clientapp.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: quantile() input tensor is too large\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 124, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 189, in process_message\n",
      "    raise ex\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 176, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=33947, ip=192.168.1.105, actor_id=7a8922abd9faa0047673b1d401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7b4ac0956c60>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/clientapp/client_app.py\", line 161, in __call__\n",
      "    return self._registered_funcs[full_name](message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/client_app.py\", line 39, in train\n",
      "    train_loss = train_sparse_fn(\n",
      "                 ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/task.py\", line 171, in train_sparse\n",
      "    masks = calibrate_gradient_masks(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/sensitivity.py\", line 94, in calibrate_gradient_masks\n",
      "    threshold = torch.quantile(all_scores, sparsity_ratio)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: quantile() input tensor is too large\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=33947, ip=192.168.1.105, actor_id=7a8922abd9faa0047673b1d401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7b4ac0956c60>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.clientapp.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: quantile() input tensor is too large\n",
      "\n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m Using cache found in /home/einrich99/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m Loading iCaRL classifier from local cache: /home/einrich99/Progetti/FL-task-arithmetic/utilities/trained/nearest_centroid_classifier.pth\n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m [Client 1] Using sparse fine-tuning\n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m === Step 1: Calibrating Gradient Masks ===\n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m Starting mask calibration with 5 rounds...\n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m Sparsity ratio: 90.00% (freezing 90.00% of parameters)\n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m Calibration round 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=33947, ip=192.168.1.105, actor_id=7a8922abd9faa0047673b1d401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7b4ac0956c60>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/clientapp/client_app.py\", line 161, in __call__\n",
      "    return self._registered_funcs[full_name](message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/client_app.py\", line 39, in train\n",
      "    train_loss = train_sparse_fn(\n",
      "                 ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/task.py\", line 171, in train_sparse\n",
      "    masks = calibrate_gradient_masks(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/sensitivity.py\", line 94, in calibrate_gradient_masks\n",
      "    threshold = torch.quantile(all_scores, sparsity_ratio)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: quantile() input tensor is too large\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=33947, ip=192.168.1.105, actor_id=7a8922abd9faa0047673b1d401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7b4ac0956c60>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.clientapp.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: quantile() input tensor is too large\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 124, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 189, in process_message\n",
      "    raise ex\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 176, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=33947, ip=192.168.1.105, actor_id=7a8922abd9faa0047673b1d401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7b4ac0956c60>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/clientapp/client_app.py\", line 161, in __call__\n",
      "    return self._registered_funcs[full_name](message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/client_app.py\", line 39, in train\n",
      "    train_loss = train_sparse_fn(\n",
      "                 ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/task.py\", line 171, in train_sparse\n",
      "    masks = calibrate_gradient_masks(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/sensitivity.py\", line 94, in calibrate_gradient_masks\n",
      "    threshold = torch.quantile(all_scores, sparsity_ratio)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: quantile() input tensor is too large\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=33947, ip=192.168.1.105, actor_id=7a8922abd9faa0047673b1d401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7b4ac0956c60>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.clientapp.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: quantile() input tensor is too large\n",
      "\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_train: Received 0 results and 2 failures\n",
      "\u001b[92mINFO \u001b[0m:      \t> Received error in reply from node 9088339682545869223: <class 'ray.exceptions.RayTaskError(ClientAppException)'>:<'\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=33947, ip=192.168.1.105, actor_id=7a8922abd9faa0047673b1d401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7b4ac0956c60>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/clientapp/client_app.py\", line 161, in __call__\n",
      "    return self._registered_funcs[full_name](message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/client_app.py\", line 39, in train\n",
      "    train_loss = train_sparse_fn(\n",
      "                 ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/task.py\", line 171, in train_sparse\n",
      "    masks = calibrate_gradient_masks(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/sensitivity.py\", line 94, in calibrate_gradient_masks\n",
      "    threshold = torch.quantile(all_scores, sparsity_ratio)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: quantile() input tensor is too large\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=33947, ip=192.168.1.105, actor_id=7a8922abd9faa0047673b1d401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7b4ac0956c60>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.clientapp.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: quantile() input tensor is too large'>\n",
      "\u001b[92mINFO \u001b[0m:      \t> Received error in reply from node 16040892530603135789: <class 'ray.exceptions.RayTaskError(ClientAppException)'>:<'\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=33947, ip=192.168.1.105, actor_id=7a8922abd9faa0047673b1d401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7b4ac0956c60>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/clientapp/client_app.py\", line 161, in __call__\n",
      "    return self._registered_funcs[full_name](message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/client_app.py\", line 39, in train\n",
      "    train_loss = train_sparse_fn(\n",
      "                 ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/task.py\", line 171, in train_sparse\n",
      "    masks = calibrate_gradient_masks(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/Progetti/FL-task-arithmetic/fl_task_arithmetic/sensitivity.py\", line 94, in calibrate_gradient_masks\n",
      "    threshold = torch.quantile(all_scores, sparsity_ratio)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: quantile() input tensor is too large\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=33947, ip=192.168.1.105, actor_id=7a8922abd9faa0047673b1d401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7b4ac0956c60>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/einrich99/anaconda3/envs/ml_flower/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.clientapp.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: quantile() input tensor is too large'>\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: Sampled 2 nodes (out of 2)\n",
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m Using cache found in /home/einrich99/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m Loading iCaRL classifier from local cache: /home/einrich99/Progetti/FL-task-arithmetic/utilities/trained/nearest_centroid_classifier.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m Using cache found in /home/einrich99/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=33947)\u001b[0m Loading iCaRL classifier from local cache: /home/einrich99/Progetti/FL-task-arithmetic/utilities/trained/nearest_centroid_classifier.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "run_config = \"conf1.toml\"\n",
    "\n",
    "command = f\"cd .. && flwr run --run-config config/{run_config}\"\n",
    "os.system(command)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_flower",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
