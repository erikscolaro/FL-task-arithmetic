{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "1S98QELEEJeU",
        "outputId": "b6a26106-dc39-45e4-a837-5e114aca9cdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:03<00:00, 42.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing CustomDino on cuda...\n",
            "Downloading: \"https://github.com/facebookresearch/dino/zipball/main\" to /root/.cache/torch/hub/main.zip\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/dino/dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth\" to /root/.cache/torch/hub/checkpoints/dino_deitsmall16_pretrain.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 82.7M/82.7M [00:00<00:00, 303MB/s]\n",
            "Epoch 1/50: 100%|██████████| 782/782 [08:29<00:00,  1.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Loss: 7.7707 | Softmax Acc: 1.09% | LR: 0.00999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/50:  63%|██████▎   | 489/782 [05:20<03:11,  1.53it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3800425510.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0mrun_baseline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3800425510.py\u001b[0m in \u001b[0;36mrun_baseline\u001b[0;34m()\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms import Compose, Normalize, ToTensor, Resize, CenterCrop\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "from typing import Optional, cast\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "# ==========================================\n",
        "# 1. Model Definition (CustomDino)\n",
        "# ==========================================\n",
        "class CustomDino(nn.Module):\n",
        "    def __init__(self, num_classes: int = 100, backbone: Optional[nn.Module] = None):\n",
        "        super().__init__()\n",
        "        if backbone is None:\n",
        "            backbone = cast(nn.Module, torch.hub.load(\n",
        "                \"facebookresearch/dino:main\", \"dino_vits16\", pretrained=True\n",
        "            ))\n",
        "        self.backbone: nn.Module = backbone\n",
        "        # We need a scalable classifier that can grow\n",
        "        self.classifier = nn.Linear(384, num_classes)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        features = self.backbone(x)        # [batch, 384]\n",
        "        logits = self.classifier(features) # [batch, num_classes]\n",
        "        return logits, features\n",
        "\n",
        "# ==========================================\n",
        "# 2. iCaRL Logic Class\n",
        "# ==========================================\n",
        "class iCaRL:\n",
        "    def __init__(self, num_classes=100, memory_size=2000, feature_dim=384, device='cuda'):\n",
        "        self.device = device\n",
        "        self.num_classes = num_classes\n",
        "        self.memory_size = memory_size\n",
        "        self.feature_dim = feature_dim\n",
        "\n",
        "        # Initialize Model\n",
        "        self.model = CustomDino(num_classes=num_classes).to(self.device)\n",
        "        self.old_model = None # Snapshot of model before current task\n",
        "\n",
        "        # Memory (Exemplars)\n",
        "        self.exemplar_sets = [] # List of lists (images per class)\n",
        "        self.exemplar_means = [] # Class prototypes for NME\n",
        "\n",
        "        # Training Parameters\n",
        "        self.lr = 0.01\n",
        "        self.weight_decay = 1e-5\n",
        "        self.momentum = 0.9\n",
        "        self.epochs = 20  # Reduced for demo speed (standard is often higher)\n",
        "\n",
        "    def update_representation(self, train_loader, new_classes):\n",
        "        \"\"\"\n",
        "        Step 1: Train the model using Classification + Distillation Loss\n",
        "        \"\"\"\n",
        "        print(f\"--- Updating Representation for classes {new_classes} ---\")\n",
        "\n",
        "        # 1. Combine new data with exemplars\n",
        "        # (In this simplified script, we assume train_loader already mixes them if available\n",
        "        # or we just iterate. For strict iCaRL, we augment the batch with exemplars).\n",
        "        # To keep it simple for Colab, we will rely on the DataLoader having the mix.\n",
        "\n",
        "        optimizer = optim.SGD(self.model.parameters(), lr=self.lr, momentum=self.momentum, weight_decay=self.weight_decay)\n",
        "        # Scheduler helps convergence\n",
        "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=self.epochs)\n",
        "\n",
        "        self.model.train()\n",
        "        if self.old_model:\n",
        "            self.old_model.eval()\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            total_loss = 0\n",
        "            for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{self.epochs}\", leave=False):\n",
        "                images = images.to(self.device)\n",
        "                labels = labels.to(self.device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward Pass\n",
        "                logits, _ = self.model(images)\n",
        "\n",
        "                # --- Loss Calculation ---\n",
        "                # A. Classification Loss (Cross Entropy on all visible classes)\n",
        "                loss_cls = F.cross_entropy(logits, labels)\n",
        "\n",
        "                # B. Distillation Loss (on OLD classes only)\n",
        "                loss_dist = torch.tensor(0.).to(self.device)\n",
        "                if self.old_model is not None:\n",
        "                    # Get old logits\n",
        "                    with torch.no_grad():\n",
        "                        old_logits, _ = self.old_model(images)\n",
        "\n",
        "                    # Sigmoid Distillation (Rebuffi et al. 2017)\n",
        "                    # We compute BCE between the sigmoid outputs of the new model and the old model\n",
        "                    # solely for the classes the old model knew.\n",
        "                    known_classes = self.old_model.classifier.out_features\n",
        "                    # Usually iCaRL assumes specific output nodes. Here we map indices.\n",
        "                    # We assume indices 0 to (start of new task) are old classes.\n",
        "\n",
        "                    # Create a mask for old classes (e.g., 0 to 10, then 0 to 20...)\n",
        "                    # The 'old_logits' typically has size [B, num_classes] same as current if architecture is fixed\n",
        "                    # Or [B, old_num_classes] if it grew. DINO linear layer is usually fixed size or grows.\n",
        "                    # Here we assume fixed size 100 for simplicity.\n",
        "\n",
        "                    # Calculate Distillation:\n",
        "                    # T=1 is standard for iCaRL's sigmoid distillation\n",
        "                    #[:, :start_new_task] Are all the old classes the new model should not forget\n",
        "                    start_new_task = new_classes[0]\n",
        "                    if start_new_task > 0:\n",
        "                        dist_target = torch.sigmoid(old_logits[:, :start_new_task])\n",
        "                        dist_pred = torch.sigmoid(logits[:, :start_new_task])\n",
        "                        loss_dist = F.binary_cross_entropy(dist_pred, dist_target)\n",
        "\n",
        "                loss = loss_cls + loss_dist\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            scheduler.step()\n",
        "            # print(f\"Epoch {epoch}: Loss {total_loss:.4f}\")\n",
        "\n",
        "        # Update the frozen old model\n",
        "        self.old_model = deepcopy(self.model)\n",
        "        for p in self.old_model.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def reduce_exemplar_sets(self, m):\n",
        "        \"\"\"\n",
        "        Step 2: Shrink stored exemplars to fit memory budget.\n",
        "        m = memory_size / num_classes_seen_so_far\n",
        "        \"\"\"\n",
        "        print(f\"Reducing exemplars to {m} per class...\")\n",
        "        for y in range(len(self.exemplar_sets)):\n",
        "            self.exemplar_sets[y] = self.exemplar_sets[y][:m]\n",
        "\n",
        "    def construct_exemplar_sets(self, images, m, transform):\n",
        "        \"\"\"\n",
        "        Step 3: Select new exemplars using Herding (nearest to mean).\n",
        "        \"\"\"\n",
        "        print(f\"Constructing exemplars (Herding)... target {m} per class\")\n",
        "        self.model.eval()\n",
        "\n",
        "        # Compute mean of the class\n",
        "        with torch.no_grad():\n",
        "            # Extract features\n",
        "            # Note: We need a loader to process 'images' (which is a list/tensor of raw images)\n",
        "            # For efficiency in this script, we assume 'images' fits in VRAM or we batch it.\n",
        "            # Simplified:\n",
        "            img_tensor = torch.stack(images).to(self.device)\n",
        "            _, features = self.model(img_tensor)\n",
        "            features = F.normalize(features, p=2, dim=1)\n",
        "            class_mean = torch.mean(features, dim=0)\n",
        "\n",
        "            # Herding Selection\n",
        "            exemplar_set = []\n",
        "            exemplar_features = []\n",
        "\n",
        "            # We assume features are [N, D]\n",
        "            # We iterate m times to pick m samples\n",
        "            for k in range(m):\n",
        "                S = torch.sum(torch.stack(exemplar_features), dim=0) if len(exemplar_features) > 0 else torch.zeros(self.feature_dim).to(self.device)\n",
        "\n",
        "                # Objective: minimize || class_mean - (S + phi(x)) / k   ||\n",
        "                phi = features # [N, D]\n",
        "                mu = class_mean # [D]\n",
        "\n",
        "                # Distance for all candidates\n",
        "                dists = torch.norm(mu - ((S + phi)/k), dim=1)\n",
        "\n",
        "                # Pick best that isn't already chosen (simple way: set dist to inf)\n",
        "                # In strict implementation, we remove the index.\n",
        "                best_idx = torch.argmin(dists).item()\n",
        "\n",
        "                exemplar_set.append(images[best_idx])\n",
        "                exemplar_features.append(features[best_idx])\n",
        "\n",
        "                # Mask this index so it's not picked again\n",
        "                features[best_idx] = features[best_idx] + 1000 # Hacky mask\n",
        "\n",
        "            self.exemplar_sets.append(exemplar_set)\n",
        "\n",
        "    def classify_nme(self, x):\n",
        "        \"\"\"\n",
        "        Step 4: Classification using Nearest Mean of Exemplars.\n",
        "        Strict Implementation of Algorithm 1 & Eq. 2\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            # 1. Get features of the image to classify\n",
        "            _, query_features = self.model(x.to(self.device))\n",
        "            # Normalize query features (Section 2.1)\n",
        "            query_features = F.normalize(query_features, p=2, dim=1)\n",
        "\n",
        "            # 2. Compute Prototypes (Means of Exemplars)\n",
        "            means = []\n",
        "            for y in range(len(self.exemplar_sets)):\n",
        "                # Get all exemplars for class y\n",
        "                ex_imgs = torch.stack(self.exemplar_sets[y]).to(self.device)\n",
        "\n",
        "                # Extract features for exemplars\n",
        "                _, ex_feats = self.model(ex_imgs)\n",
        "\n",
        "                # Normalize exemplar features BEFORE averaging (Section 2.1)\n",
        "                ex_feats = F.normalize(ex_feats, p=2, dim=1)\n",
        "\n",
        "                # Compute the mean\n",
        "                class_mean = torch.mean(ex_feats, dim=0)\n",
        "\n",
        "                # Re-normalize the mean vector itself (Section 2.1: \"averages are also re-normalized\")\n",
        "                class_mean = F.normalize(class_mean.unsqueeze(0), p=2, dim=1).squeeze(0)\n",
        "\n",
        "                means.append(class_mean)\n",
        "\n",
        "            if len(means) == 0: return torch.zeros(x.size(0))\n",
        "\n",
        "            means = torch.stack(means) # [Num_Classes_Seen, Feature_Dim]\n",
        "\n",
        "            # 3. Find Nearest Prototype (Algorithm 1)\n",
        "            # \"y* = argmin || phi(x) - mu_y ||\"\n",
        "            dists = torch.cdist(query_features, means) # [Batch, Num_Classes]\n",
        "            preds = torch.argmin(dists, dim=1)\n",
        "\n",
        "        return preds\n",
        "\n",
        "# ==========================================\n",
        "# 3. Data Utilities\n",
        "# ==========================================\n",
        "class iCaRLDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset that combines new task data with stored exemplars.\n",
        "    \"\"\"\n",
        "    def __init__(self, new_data, exemplars, transform=None):\n",
        "        self.new_data = new_data # List of (image, label) tuples\n",
        "        self.exemplars = exemplars # List of lists of images\n",
        "        self.transform = transform\n",
        "\n",
        "        # Flatten exemplars into a list of (img, label)\n",
        "        self.exemplar_data = []\n",
        "        for label, img_list in enumerate(exemplars):\n",
        "            for img in img_list:\n",
        "                self.exemplar_data.append((img, label))\n",
        "\n",
        "        self.all_data = self.new_data + self.exemplar_data\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, label = self.all_data[index]\n",
        "        # img is a Tensor here if coming from CIFAR100(ToTensor),\n",
        "        # but iCaRL usually stores raw images.\n",
        "        # For simplicity in this script, we assume img is already Tensor from prev loader\n",
        "        # If transform is needed, apply here.\n",
        "        return img, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_data)\n",
        "\n",
        "def get_data_for_classes(dataset, classes):\n",
        "    \"\"\"\n",
        "    Extracts all samples belonging to specific classes.\n",
        "    \"\"\"\n",
        "    indices = [i for i, label in enumerate(dataset.targets) if label in classes]\n",
        "    return Subset(dataset, indices)\n",
        "\n",
        "def extract_images_from_subset(subset):\n",
        "    \"\"\"\n",
        "    Helper to pull images out of a Subset for exemplar storage.\n",
        "    \"\"\"\n",
        "    images = []\n",
        "    # This is slow for large sets, efficient implementation would use indices directly\n",
        "    # But for a tutorial script, iterating is safe.\n",
        "    for i in range(len(subset)):\n",
        "        img, _ = subset[i]\n",
        "        images.append(img)\n",
        "    return images\n",
        "\n",
        "# ==========================================\n",
        "# 4. Main Experiment Loop\n",
        "# ==========================================\n",
        "def main():\n",
        "    print(\"Preparing Data...\")\n",
        "    # Transforms\n",
        "    stats = ((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "    transform = Compose([\n",
        "        Resize(256), CenterCrop(224),\n",
        "        ToTensor(),\n",
        "        Normalize(*stats),\n",
        "    ])\n",
        "\n",
        "    # Load FULL Datasets\n",
        "    train_ds = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "    test_ds = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "    # Initialize iCaRL\n",
        "    icarl = iCaRL(num_classes=100, memory_size=2000, device='cuda')\n",
        "\n",
        "    # Define Tasks (e.g., 5 tasks of 20 classes each)\n",
        "    TASKS = 5\n",
        "    CLASSES_PER_TASK = 100 // TASKS\n",
        "\n",
        "    accuracies = []\n",
        "\n",
        "    for task_id in range(TASKS):\n",
        "        # 1. Define Classes for this Task\n",
        "        start_class = task_id * CLASSES_PER_TASK\n",
        "        end_class = (task_id + 1) * CLASSES_PER_TASK\n",
        "        new_classes = list(range(start_class, end_class))\n",
        "\n",
        "        print(f\"\\n================ TASK {task_id+1}/{TASKS} : Classes {new_classes} ================\")\n",
        "\n",
        "        # 2. Prepare Training Data (New Data + Exemplars)\n",
        "        # Get subset of ONLY new classes\n",
        "        task_data_subset = get_data_for_classes(train_ds, new_classes)\n",
        "\n",
        "        # Create a list of (img, label) for the custom dataset\n",
        "        # We iterate once to cache them (RAM intensive but simpler code)\n",
        "        new_data_list = []\n",
        "        for i in range(len(task_data_subset)):\n",
        "            img, target = task_data_subset[i]\n",
        "            new_data_list.append((img, target))\n",
        "\n",
        "        # Create Hybrid Dataset\n",
        "        train_dataset = iCaRLDataset(new_data_list, icarl.exemplar_sets)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "        # 3. Train (Update Representation)\n",
        "        icarl.update_representation(train_loader, new_classes)\n",
        "\n",
        "        # 4. Exemplar Management\n",
        "        # A. Reduce old sets\n",
        "        m = icarl.memory_size // end_class\n",
        "        icarl.reduce_exemplar_sets(m)\n",
        "\n",
        "        # B. Construct new sets\n",
        "        for c in new_classes:\n",
        "            # Extract images for specific class c\n",
        "            # (Re-extract from subset for clean separation)\n",
        "            class_subset = get_data_for_classes(train_ds, [c])\n",
        "            images_c = extract_images_from_subset(class_subset)\n",
        "            icarl.construct_exemplar_sets(images_c, m, transform)\n",
        "\n",
        "        # 5. Evaluate on ALL classes seen so far\n",
        "        print(\"Evaluating...\")\n",
        "        seen_classes = list(range(0, end_class))\n",
        "        test_subset = get_data_for_classes(test_ds, seen_classes)\n",
        "        test_loader = DataLoader(test_subset, batch_size=64, shuffle=False)\n",
        "\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for imgs, lbls in tqdm(test_loader):\n",
        "            imgs = imgs.to(icarl.device)\n",
        "            lbls = lbls.to(icarl.device)\n",
        "            preds = icarl.classify_nme(imgs)\n",
        "            correct += preds.eq(lbls).sum().item()\n",
        "            total += lbls.size(0)\n",
        "\n",
        "        acc = 100. * correct / total\n",
        "        accuracies.append(acc)\n",
        "        print(f\"Task {task_id+1} Accuracy (NME): {acc:.2f}%\")\n",
        "\n",
        "    print(\"\\nFinal Accuracies per Task:\", accuracies)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}