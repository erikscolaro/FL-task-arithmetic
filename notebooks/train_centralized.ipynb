{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f7d5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madrientrahan\u001b[0m (\u001b[33maml-fl-project\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/adrientrahan/Documents/ecole/AML/project/FL-task-arithmetic/notebooks/wandb/run-20251208_121432-run-1-centralized-dino-icarl-cifar100-lr0.01-mom0.9-wd0.0005-sched-CosineAnnealingLR</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aml-fl-project/fl-task-arithmetic/runs/run-1-centralized-dino-icarl-cifar100-lr0.01-mom0.9-wd0.0005-sched-CosineAnnealingLR' target=\"_blank\">centralized-dino-icarl-cifar100-lr0.01-mom0.9-wd0.0005-sched-CosineAnnealingLR</a></strong> to <a href='https://wandb.ai/aml-fl-project/fl-task-arithmetic' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aml-fl-project/fl-task-arithmetic' target=\"_blank\">https://wandb.ai/aml-fl-project/fl-task-arithmetic</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aml-fl-project/fl-task-arithmetic/runs/run-1-centralized-dino-icarl-cifar100-lr0.01-mom0.9-wd0.0005-sched-CosineAnnealingLR' target=\"_blank\">https://wandb.ai/aml-fl-project/fl-task-arithmetic/runs/run-1-centralized-dino-icarl-cifar100-lr0.01-mom0.9-wd0.0005-sched-CosineAnnealingLR</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/adrientrahan/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">centralized-dino-icarl-cifar100-lr0.01-mom0.9-wd0.0005-sched-CosineAnnealingLR</strong> at: <a href='https://wandb.ai/aml-fl-project/fl-task-arithmetic/runs/run-1-centralized-dino-icarl-cifar100-lr0.01-mom0.9-wd0.0005-sched-CosineAnnealingLR' target=\"_blank\">https://wandb.ai/aml-fl-project/fl-task-arithmetic/runs/run-1-centralized-dino-icarl-cifar100-lr0.01-mom0.9-wd0.0005-sched-CosineAnnealingLR</a><br> View project at: <a href='https://wandb.ai/aml-fl-project/fl-task-arithmetic' target=\"_blank\">https://wandb.ai/aml-fl-project/fl-task-arithmetic</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251208_121432-run-1-centralized-dino-icarl-cifar100-lr0.01-mom0.9-wd0.0005-sched-CosineAnnealingLR/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/adrientrahan/Documents/ecole/AML/project/FL-task-arithmetic/notebooks/wandb/run-20251208_121434-run-1-icarl_cifar100</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/aml-fl-project/fl-task-arithmetic/runs/run-1-icarl_cifar100' target=\"_blank\">iCaRL_CIFAR100</a></strong> to <a href='https://wandb.ai/aml-fl-project/fl-task-arithmetic' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aml-fl-project/fl-task-arithmetic' target=\"_blank\">https://wandb.ai/aml-fl-project/fl-task-arithmetic</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aml-fl-project/fl-task-arithmetic/runs/run-1-icarl_cifar100' target=\"_blank\">https://wandb.ai/aml-fl-project/fl-task-arithmetic/runs/run-1-icarl_cifar100</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/adrientrahan/.cache/torch/hub/facebookresearch_dino_main\n",
      "Using cache found in /Users/adrientrahan/.cache/torch/hub/facebookresearch_dino_main\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact 'icarl-cifar100-checkpoints:latest', 596.51MB. 1 files...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 00:03:10.8 (3.1MB/s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /Users/adrientrahan/Documents/ecole/AML/project/FL-task-arithmetic/notebooks/artifacts/icarl-cifar100-checkpoints:v14/model.pth\n",
      "Successfully loaded model from: /Users/adrientrahan/Documents/ecole/AML/project/FL-task-arithmetic/notebooks/artifacts/icarl-cifar100-checkpoints:v14/model.pth\n",
      "Run (run-1-centralized-dino-icarl-cifar100-lr0.01-mom0.9-wd0.0005-sched-CosineAnnealingLR) is finished. The call to `use_artifact` will be ignored. Please make sure that you are using an active run.\n",
      "Model checkpoint not found on WandB. Run (run-1-centralized-dino-icarl-cifar100-lr0.01-mom0.9-wd0.0005-sched-CosineAnnealingLR) is finished. The call to `use_artifact` will be ignored. Please make sure that you are using an active run.\n",
      "Starting from scratch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 782/782 [10:35<00:00,  1.23it/s, loss=4.77]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Results: Train Loss: 8.9377 | Test Loss: 5.7398 | Test Acc: 2.02%\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'cpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 158\u001b[0m\n\u001b[1;32m    151\u001b[0m SCHEDULERS \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    152\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCosineAnnealingLR\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m opt: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mCosineAnnealingLR(opt, T_max\u001b[38;5;241m=\u001b[39mEPOCHS)),\n\u001b[1;32m    153\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStepLR\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m opt: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mStepLR(opt, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)),\n\u001b[1;32m    154\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNoScheduler\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m opt: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mLambdaLR(opt, lr_lambda\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m epoch: \u001b[38;5;241m1.0\u001b[39m)) \u001b[38;5;66;03m# No scheduling\u001b[39;00m\n\u001b[1;32m    155\u001b[0m ]\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m scheduler_name, scheduler_fn \u001b[38;5;129;01min\u001b[39;00m SCHEDULERS:\n\u001b[0;32m--> 158\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMOMENTUM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mWEIGHT_DECAY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscheduler_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscheduler_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler_fn\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 137\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(lr, momentum, weight_decay, epochs, scheduler_name, scheduler_fn)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (acc \u001b[38;5;241m>\u001b[39m best_accuracy):\n\u001b[1;32m    135\u001b[0m     best_accuracy \u001b[38;5;241m=\u001b[39m acc\n\u001b[1;32m    136\u001b[0m     save_checkpoint_to_wandb(run, {\n\u001b[0;32m--> 137\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m(),\n\u001b[1;32m    138\u001b[0m     }, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: acc,\n\u001b[1;32m    141\u001b[0m     })\n\u001b[1;32m    142\u001b[0m     patience_counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'cpu'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor, Resize, CenterCrop\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from typing import Optional, cast\n",
    "import wandb\n",
    "from models.dino_icarl import DinoIcarlModel\n",
    "from utilities.wandb_utils import load_checkpoint_from_wandb, save_checkpoint_to_wandb\n",
    "\n",
    "ENTITY = \"aml-fl-project\"\n",
    "PROJECT = \"fl-task-arithmetic\"\n",
    "GROUP = \"centralized-dino-cifar100\"\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "LR  = 0.01           # Learning Rate (Tune this: 0.1, 0.01, 0.001)\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 5e-4\n",
    "EPOCHS = 100         # Increase to 100+ for final results\n",
    "DEVICE = torch.device(\"mps\") # torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "PATIENCE = 3\n",
    "\n",
    "# Standard CIFAR-100 Normalization\n",
    "stats = ((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
    "\n",
    "# Transforms\n",
    "transform_train = Compose([\n",
    "    Resize(256), CenterCrop(224), # Required for DINO\n",
    "    # transforms.RandomHorizontalFlip(), # Optional augmentation\n",
    "    ToTensor(),\n",
    "    Normalize(*stats),\n",
    "])\n",
    "\n",
    "transform_test = Compose([\n",
    "    Resize(256), CenterCrop(224),\n",
    "    ToTensor(),\n",
    "    Normalize(*stats),\n",
    "])\n",
    "\n",
    "trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
    "testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "testloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "def train(lr, momentum, weight_decay, epochs, scheduler_name, scheduler_fn):\n",
    "    best_accuracy = 0.0\n",
    "    patience_counter = 0\n",
    "    run_id = f\"run-1-centralized-dino-icarl-cifar100-lr{lr}-mom{momentum}-wd{weight_decay}-sched-{scheduler_name}\"\n",
    "    run = wandb.init(\n",
    "        entity=ENTITY,\n",
    "        project=PROJECT,\n",
    "        group=GROUP,\n",
    "        name=f\"centralized-dino-icarl-cifar100-lr{lr}-mom{momentum}-wd{weight_decay}-sched-{scheduler_name}\",\n",
    "        id=run_id,\n",
    "        resume=\"allow\",\n",
    "        mode=\"online\",\n",
    "    )\n",
    "\n",
    "    model = DinoIcarlModel(device=DEVICE).to(DEVICE)\n",
    "\n",
    "    checkpoint = load_checkpoint_from_wandb(\n",
    "        run,\n",
    "        model,\n",
    "        \"model.pth\"\n",
    "    )\n",
    "    start_epoch = 0\n",
    "    if checkpoint is not None:\n",
    "        checkpoint_dict, artifact = checkpoint\n",
    "        model.load_state_dict(checkpoint_dict['model'])\n",
    "        start_epoch = artifact.metadata[\"epoch\"] + 1\n",
    "        print(f\"Resuming from epoch {start_epoch}\")\n",
    "    else:\n",
    "        print(\"Starting from scratch\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    scheduler = scheduler_fn(optimizer)\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    test_accs = []\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Progress bar for training\n",
    "        pbar = tqdm(trainloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_train_loss = running_loss / len(trainloader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # 5. Evaluation\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in testloader:\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        avg_test_loss = test_loss / len(testloader)\n",
    "        acc = 100. * correct / total\n",
    "\n",
    "        test_losses.append(avg_test_loss)\n",
    "        test_accs.append(acc)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} Results: Train Loss: {avg_train_loss:.4f} | Test Loss: {avg_test_loss:.4f} | Test Acc: {acc:.2f}%\")\n",
    "\n",
    "        if (acc > best_accuracy):\n",
    "            best_accuracy = acc\n",
    "            save_checkpoint_to_wandb(run, {\n",
    "                'model': model.state_dict(),\n",
    "            }, f\"model.pth\", {\n",
    "                \"task\": model,\n",
    "                \"accuracy\": acc,\n",
    "            })\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter > PATIENCE:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "        print(epoch, \"Saved checkpoint model to WandB.\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf3afac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SCHEDULERS = [\n",
    "    (\"CosineAnnealingLR\", lambda opt: torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS)),\n",
    "    (\"StepLR\", lambda opt: torch.optim.lr_scheduler.StepLR(opt, step_size=30, gamma=0.1)),\n",
    "    (\"NoScheduler\", lambda opt: torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda=lambda epoch: 1.0)) # No scheduling\n",
    "]\n",
    "    \n",
    "\n",
    "for scheduler_name, scheduler_fn in SCHEDULERS:\n",
    "    train(\n",
    "        lr=LR,\n",
    "        momentum=MOMENTUM,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        epochs=EPOCHS, \n",
    "        scheduler_name=scheduler_name, \n",
    "        scheduler_fn=scheduler_fn\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f026b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATES = [0.1, 0.01, 0.001]\n",
    "\n",
    "for lr in LEARNING_RATES:\n",
    "    train(\n",
    "        lr=lr,\n",
    "        momentum=MOMENTUM,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        epochs=EPOCHS, \n",
    "        scheduler_name=SCHEDULERS[0][0], \n",
    "        scheduler_fn=SCHEDULERS[0][1]\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl-task-arithmetic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
