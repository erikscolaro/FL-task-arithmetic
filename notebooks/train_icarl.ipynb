{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4364560f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">iCaRL_CIFAR100</strong> at: <a href='https://wandb.ai/aml-fl-project/fl-task-arithmetic/runs/run-1-icarl_cifar100' target=\"_blank\">https://wandb.ai/aml-fl-project/fl-task-arithmetic/runs/run-1-icarl_cifar100</a><br> View project at: <a href='https://wandb.ai/aml-fl-project/fl-task-arithmetic' target=\"_blank\">https://wandb.ai/aml-fl-project/fl-task-arithmetic</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251206_163637-run-1-icarl_cifar100/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/adrientrahan/Documents/ecole/AML/project/FL-task-arithmetic/notebooks/wandb/run-20251206_164048-run-1-icarl_cifar100</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/aml-fl-project/fl-task-arithmetic/runs/run-1-icarl_cifar100' target=\"_blank\">iCaRL_CIFAR100</a></strong> to <a href='https://wandb.ai/aml-fl-project/fl-task-arithmetic' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aml-fl-project/fl-task-arithmetic' target=\"_blank\">https://wandb.ai/aml-fl-project/fl-task-arithmetic</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aml-fl-project/fl-task-arithmetic/runs/run-1-icarl_cifar100' target=\"_blank\">https://wandb.ai/aml-fl-project/fl-task-arithmetic/runs/run-1-icarl_cifar100</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/adrientrahan/.cache/torch/hub/facebookresearch_dino_main\n",
      "Using cache found in /Users/adrientrahan/.cache/torch/hub/facebookresearch_dino_main\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact 'icarl-cifar100-checkpoints:latest', 82.85MB. 1 files...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 00:00:00.4 (202.6MB/s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/Users/adrientrahan/Documents/ecole/AML/project/FL-task-arithmetic/notebooks/artifacts/icarl-cifar100-checkpoints:v12/icarl-0.pth'\n",
      "Model checkpoint not found on WandB. [Errno 2] No such file or directory: '/Users/adrientrahan/Documents/ecole/AML/project/FL-task-arithmetic/notebooks/artifacts/icarl-cifar100-checkpoints:v12/icarl-0.pth'\n",
      "Starting from scratch\n",
      "\n",
      "================ TASK 1/20 : Classes [0, 1, 2, 3, 4] ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss 112.9798\n",
      "Reducing exemplars to 10 per class...\n",
      "Constructing 10 exemplars vectors per class number 0\n",
      "Constructing 10 exemplars vectors per class number 1\n",
      "Constructing 10 exemplars vectors per class number 2\n",
      "Constructing 10 exemplars vectors per class number 3\n",
      "Constructing 10 exemplars vectors per class number 4\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:20<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 Accuracy (NME): 90.80%\n",
      "Model saved to WandB as artifact 'icarl-cifar100-checkpoints'.\n",
      "0 Saved checkpoint model to WandB.\n",
      "\n",
      "================ TASK 2/20 : Classes [5, 6, 7, 8, 9] ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  30%|███       | 12/40 [00:40<01:28,  3.14s/it]"
     ]
    }
   ],
   "source": [
    "from data.icarl_dataset import iCaRLDataset, get_data_for_classes, extract_images_from_subset\n",
    "from time import sleep\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor, Resize, CenterCrop\n",
    "from torchvision import datasets\n",
    "import torch\n",
    "from models.icarl_head import IcarlModel, Icarl\n",
    "import wandb\n",
    "from utilities.wandb_utils import save_model_to_wandb, load_model_from_wandb\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "RUN_ID = \"run-1-icarl_cifar100\"\n",
    "ENTITY = \"aml-fl-project\"\n",
    "PROJECT = \"fl-task-arithmetic\"\n",
    "GROUP = \"icarl-cifar100\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "TOTAL_EXEMPLARS_VECTORS = 1000\n",
    "TASKS = 20\n",
    "CLASSES_PER_TASK = 100 // TASKS\n",
    "\n",
    "EPOCHS = 20\n",
    "LR = 0.01\n",
    "WEIGHT_DECAY = 1e-5\n",
    "MOMENTUM = 0.9\n",
    "\n",
    "\n",
    "stats = ((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
    "transform = Compose([\n",
    "    Resize(256), CenterCrop(224),\n",
    "    ToTensor(),\n",
    "    Normalize(*stats),\n",
    "])\n",
    "\n",
    "# Load FULL Datasets\n",
    "train_ds = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "test_ds = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "run = wandb.init(\n",
    "    entity=ENTITY,\n",
    "    project=PROJECT,\n",
    "    group=GROUP,\n",
    "    name=\"iCaRL_CIFAR100\",\n",
    "    id=RUN_ID,\n",
    "    resume=\"allow\",\n",
    "    mode=\"online\",\n",
    ")\n",
    "\n",
    "# Initialize iCaRL\n",
    "icarl = Icarl(\n",
    "    num_classes=100,\n",
    "    memory_size=TOTAL_EXEMPLARS_VECTORS,\n",
    "    device=DEVICE\n",
    ")\n",
    "icarl.old_model = IcarlModel(num_classes=100)\n",
    "\n",
    "artifact = load_model_from_wandb(\n",
    "    run,\n",
    "    icarl,\n",
    "    \"icarl-0.pth\"\n",
    ")\n",
    "\n",
    "start_task = 0\n",
    "if artifact is not None and \"task\" in artifact.metadata:\n",
    "    start_task = artifact.metadata[\"task\"] + 1\n",
    "    print(f\"Resuming from task {start_task}\")\n",
    "else:\n",
    "    icarl.old_model = None\n",
    "    print(\"Starting from scratch\")  \n",
    "\n",
    "for task_id in range(start_task, TASKS):\n",
    "    # 1. Define Classes for this Task\n",
    "    start_class = task_id * CLASSES_PER_TASK\n",
    "    end_class = (task_id + 1) * CLASSES_PER_TASK\n",
    "    new_classes = list(range(start_class, end_class))\n",
    "\n",
    "    print(f\"\\n================ TASK {task_id+1}/{TASKS} : Classes {new_classes} ================\")\n",
    "\n",
    "    # 2. Prepare Training Data (New Data + Exemplars)\n",
    "    # Get subset of ONLY new classes\n",
    "    task_data_subset = get_data_for_classes(train_ds, new_classes)\n",
    "\n",
    "    # Create a list of (img, label) for the custom dataset\n",
    "    # We iterate once to cache them (RAM intensive but simpler code)\n",
    "    new_data_list = []\n",
    "    for i in range(len(task_data_subset)):\n",
    "        img, target = task_data_subset[i]\n",
    "        new_data_list.append((img, target))\n",
    "\n",
    "    # Create Hybrid Dataset\n",
    "    train_dataset = iCaRLDataset(new_data_list, icarl.exemplar_sets)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "    # 3. Train (Update Representation) \n",
    "    #Only train the classifier parameters\n",
    "    optimizer = optim.SGD(icarl.model.classifier.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "    # Scheduler helps convergence\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "    icarl.model.train()\n",
    "    if icarl.old_model:\n",
    "        icarl.old_model.eval()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss = 0\n",
    "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", leave=False):\n",
    "            images = images.to(icarl.device)\n",
    "            labels = labels.to(icarl.device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward Pass\n",
    "            logits, _ = icarl.model(images)\n",
    "\n",
    "            # --- Loss Calculation ---\n",
    "            # A. Classification Loss (Cross Entropy on all visible classes)\n",
    "            loss_cls = F.cross_entropy(logits, labels)\n",
    "\n",
    "            # B. Distillation Loss (on OLD classes only)\n",
    "            loss_dist = torch.tensor(0.).to(icarl.device)\n",
    "            if icarl.old_model is not None:\n",
    "                # Get old logits\n",
    "                with torch.no_grad():\n",
    "                    old_logits, _ = icarl.old_model(images)\n",
    "\n",
    "                # Sigmoid Distillation (Rebuffi et al. 2017)\n",
    "                # We compute BCE between the sigmoid outputs of the new model and the old model\n",
    "                # solely for the classes the old model knew.\n",
    "                known_classes = icarl.old_model.classifier.out_features\n",
    "                # Usually iCaRL assumes specific output nodes. Here we map indices.\n",
    "                # We assume indices 0 to (start of new task) are old classes.\n",
    "\n",
    "                # Create a mask for old classes (e.g., 0 to 10, then 0 to 20...)\n",
    "                # The 'old_logits' typically has size [B, num_classes] same as current if architecture is fixed\n",
    "                # Or [B, old_num_classes] if it grew. DINO linear layer is usually fixed size or grows.\n",
    "                # Here we assume fixed size 100 for simplicity.\n",
    "\n",
    "                # Calculate Distillation:\n",
    "                # T=1 is standard for iCaRL's sigmoid distillation\n",
    "                #[:, :start_new_task] Are all the old classes the new model should not forget\n",
    "                start_new_task = new_classes[0]\n",
    "                if start_new_task > 0:\n",
    "                    dist_target = torch.sigmoid(old_logits[:, :start_new_task])\n",
    "                    dist_pred = torch.sigmoid(logits[:, :start_new_task])\n",
    "                    loss_dist = F.binary_cross_entropy(dist_pred, dist_target)\n",
    "\n",
    "            loss = loss_cls + loss_dist\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch}: Loss {total_loss:.4f}\")\n",
    "\n",
    "    # 4. Exemplar Management\n",
    "    # A. Reduce old sets\n",
    "    m = TOTAL_EXEMPLARS_VECTORS // 100\n",
    "    icarl.reduce_exemplar_sets(m)\n",
    "\n",
    "    # B. Construct new sets\n",
    "    for c in new_classes:\n",
    "        # Extract images for specific class c\n",
    "        # (Re-extract from subset for clean separation)\n",
    "        class_subset = get_data_for_classes(train_ds, [c])\n",
    "        images_c = extract_images_from_subset(class_subset)\n",
    "        icarl.construct_exemplar_sets(images_c, m, transform,c)\n",
    "\n",
    "    # 5. Evaluate on ALL classes seen so far\n",
    "    print(\"Evaluating...\")\n",
    "    seen_classes = list(range(0, end_class))\n",
    "    test_subset = get_data_for_classes(test_ds, seen_classes)\n",
    "    test_loader = DataLoader(test_subset, batch_size=64, shuffle=False)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for imgs, lbls in tqdm(test_loader):\n",
    "        imgs = imgs.to(icarl.device)\n",
    "        lbls = lbls.to(icarl.device)\n",
    "        logits, _ = icarl(imgs)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += preds.eq(lbls).sum().item()\n",
    "        total += lbls.size(0)\n",
    "\n",
    "    acc = 100. * correct / total\n",
    "    print(f\"Task {task_id+1} Accuracy (NME): {acc:.2f}%\")\n",
    "\n",
    "\n",
    "    save_model_to_wandb(run, icarl, f\"../checkpoints/icarl-{task_id}.pth\", {\n",
    "        \"task\": task_id,\n",
    "        \"accuracy\": acc,\n",
    "    })\n",
    "    print(task_id, \"Saved checkpoint model to WandB.\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl-task-arithmetic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
